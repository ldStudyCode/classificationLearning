[TOC]

> 参考书籍：
>
> 1. 《深入理解Java虚拟机》12、13章
> 2. 《Java并发编程实战》
> 3. 《Java虚拟机并发编程》第二章



# 1. 线程安全

## 1.1 定义

线程安全的定义：

当多线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。

换个说法，多线程以任意的顺序来执行、线程之间互相穿插执行，最终的结果就跟串行执行是一样的，就可以称为线程安全。

## 1.2 线程安全级别

线程安全级别分为如下5类，安全性由强至弱排序如下：

### 1.2.1 不可变

不可变的对象一定线程安全。不可变的对象一旦被构造出来，永远也不会产生多线程中不一致的现象。

Java实现不可变的方法：

* 8种基本类型：加final；
* 对象：把其中所有属性加final。
  * 例：String，和Integer、Double等数字类都是不可变对象。

### 1.2.2 绝对线程安全

不管运行时环境如何，调用者都不需要任何额外的同步措施。

绝对线程安全并不是我们通常意义上的线程安全，因为它不光要保证单次调用下的安全，还要保证连续调用下的安全。而在调用方没有其他保护的情况下，这基本上是不可能实现的。

例如，对于一个 `AtomicInteger` 对象 `i` ，若多个线程同时执行 `i = i.get() + 1` ，是无法达到线程安全的，因为这个操作会先取值、再加1、最后赋值，这三步操作会在多线程下交叉执行，一定会产生线程安全问题。

改进的方法是，将 `i = i.get() + 1` 语句替换为 `i.getAndAdd(1)` ，将三步操作放到一个原子性操作中去。

### 1.2.3 相对线程安全

相对线程安全，就是我们通常意义上所讲的线程安全。它需要保证对这个对象**单独操作**是线程安全的，但对于一些特定顺序的连续调用，需要在调用端使用额外的同步手段来保证正确性。

Java中大多数声明为线程安全的类，指的都是这一类。

### 1.2.4 线程兼容

指对象本身不是线程安全的，但可以通过调用方正确使用同步手段，来保证对象在并发环境下安全使用。

Java中大部分没有声明为线程安全的类，指的都是这一类。

### 1.2.5 线程对立

无论调用方是否采取同步措施，都无法在多线程环境中安全地并发使用。

由于Java语言天生具备多线程的特性，这一类在Java中基本不会出现。



## 1.3 线程安全的实现方法

### 1.3.1 互斥同步

#### 1.3.1.1 互斥同步的特点

互斥同步保证同一时刻只被一个线程使用。Java中，synchronized关键字和ReentrantLock都可以实现互斥。

互斥同步又称“阻塞同步”，因为抢不到锁的线程会阻塞。而线程的阻塞和唤醒是需要OS进入内核态的，会有性能问题。

从处理方式上看，互斥同步是一种悲观锁。它认为，只要不上锁，一定会出现问题，无论共享数据是否真的会出现竞争。跟它对应的是非阻塞同步，是一种乐观锁策略。

#### 1.3.1.2 synchronized

* **锁住了谁？**

  若代码中明确了锁对象，锁住的就是这个对象；

  修饰了一个方法：若是静态方法，则锁住了Class对象；若是实例方法，则锁住了这个对象实例；

* **上锁的流程？**

  synchronized关键字编译后，会在同步块的前后分别形成moniterenter和moniterexit这两个字节码指令，这两个指令内部执行的，就是后面要讲的CPU原子操作：lock和unlock。

  执行moniterenter时，首先尝试获取对象的锁：若对象没有被锁定，或当前线程已经拥有了这个对象的锁（此处表示可重入），则锁的计数器+1。相应的，在执行moniterexit时，会将计数器-1。计数器为0时，锁被释放。

  若获取对象锁失败，则线程阻塞，直到对象锁被另外一个线程释放，且自己抢到为止（非公平）。

* **重量级锁**

  Java线程是1:1映射到操作系统原生线程之上的，因此如果要阻塞或唤醒一个线程，都需要操作系统帮忙完成。这就需要操作系统从用户态转换到内核态中，而这个操作是开销比较大的。所以说synchronized是一个重量级锁。

  为了优化synchronized的性能，JVM会在阻塞线程之前加入一段自旋等待的过程，避免频繁切入到内核态中。将在下面章节讲到。

#### 1.3.1.3 ReentrantLock

与synchronized作用相似，也是可重入锁，但提供更多可配置选项。ReentrantLock的标准使用形式如下，注意一定要在finally块中解锁，否则业务逻辑中抛出异常，将导致锁无法被释放：

```java
Lock lock = new ReentrantLock();
lock.lock();
try {
    // ...
} finally {
    lock.unlock();
}
```

相比于synchronized的一些高级功能如下。

##### 1.3.1.3.1 等待可中断

等待中断，意思是不会像synchronized一样，不获取到锁不罢休。ReentrantLock提供以下方法，来实现等待中断：

* `tryLock()` ：尝试获取一次锁，获取不到则返回失败标志。可用于实现轮询锁，以解决死锁问题。见7.2.2节互相转账死锁问题的解决方法；
* `tryLock(timeout)` ：尝试获取锁一段时间，获取不到则返回失败标志；
* `lockInterruptibly()` ：在抢占锁的过程中，能保持对中断的响应。即其他线程调用该线程的 `interrupt()` 方法，此方法能感知到并中断；
* `Condition#await(timeout)` ：定时阻塞方法，而synchronized中只能使用 `Object#wait()` 方法无限阻塞。注意它与 `Thread#sleep()` 不同，wait方法会释放锁，唤醒后会重新抢夺锁，而sleep会持续占有锁。

##### 1.3.1.3.2 公平锁

默认非公平锁，但可通过构造函数设置为公平锁。即等待时间越长的线程，越先从阻塞中被唤醒。

在公平锁中，如果有另一个线程持有这个锁，或有其他线程在队列中等待这个锁，那么新发出请求的线程将被放入队列中；而在非公平锁中，只有当锁被某个线程持有时，新发出请求的线程才会被放入队列中。

多数情况下，非公平的锁性能要高于公平锁的性能，因为非公平锁是允许“插队”的，非公平的机制节省了那些插队线程的阻塞唤醒时间。但公平锁可能会产生线程饥饿的情况。

因此，若持有锁的时间相对较长，或请求锁的平均时间间隔较长，应使用公平锁。而并发高是应该使用默认的非公平锁。

##### 1.3.1.3.3 绑定多个条件

利用锁创建多个Condition对象，给阻塞中的线程分组。实例：单锁的阻塞队列 `LinkedBlockingDequeue` ，同一把锁，分为了put和take两个阻塞条件，可指定唤醒某一组中的一个线程。

#### 1.3.1.4 二者比较

synchronized关键字和显示锁ReentrantLock，都提供了原子性、可见性、有序性的语义。

从性能上看，早期（jdk1.5）ReentrantLock的性能更好，但在jdk1.6之后，对synchronized做了锁消除、锁粗化等各种优化，因此二者吞吐量非常接近。又考虑到，ReentrantLock编写较麻烦，容易出错，因此书中的结论是，仅当synchronized不能满足需求时，才可以考虑使用ReentrantLock。



### 1.3.2 非阻塞同步

非阻塞同步是一种基于冲突检测的乐观锁。具体操作是：先不上锁，直接操作，如果没有其他线程竞争共享数据，那么操作就成功了；如果共享数据有竞争，就采取补偿措施（不断重试，直到成功为止）。

按照这种逻辑，非阻塞同步不会让线程阻塞、唤醒，相比于阻塞同步提高了性能。

为了实现非阻塞同步，必须保证“冲突检测+操作”的步骤具有原子性。但如果这里再使用互斥同步就失去非阻塞的意义了，因此只能靠CPU指令来保证这件事。CPU提供了CAS操作（compare and swap，比较并交换），比较就是冲突检测，交换就是操作。

一个利用CAS实现计数器+1的方法实现如下（AtomicInteger中的实现类似）：

```java
increment() {
  while(true) {
    int current = get(); // 当前数值，即待比较的数值
    int next = current + 1; // 计算后的结果
    if(cas(current, next)) { // 调用CPU保证的原子性cas操作
      return;
    }
  }
}
```

CAS操作的一点小问题：ABA问题。初次读取时是A，比较时仍然也是A，但可能在读取后、比较前，变量值从A改成了B，又迅速改回了A。CAS操作就会误认为它是没有被改变过。但大部分情况下，ABA问题不会影响程序并发的正确性，因此还是可以放心使用CAS的。



### 1.3.3 无同步方案

如果一段代码天生就是线程安全的，也就不需要通过特殊手段来保证同步了。有如下三类：

* 不可变对象：final的8大基本类型数据，以及成员变量都是final的对象，它们一定是线程安全的；

* 可重入代码：不依赖堆内存上的共享资源，所有的状态都是存在于线程独立的栈内存中的；
* 线程本地存储：如果能保证数据只在同一个线程内被使用，而不会泄露给其他线程，这样也是可以保证安全的。常见的场景有：
  * Web交互模型中的“一个请求对应一个服务器线程”，这样我们在写服务器端的Controller方法时，无需考虑线程安全问题；
  * ThreadLocal：实现线程本地存储，每个线程只操作自己的变量，而不会泄露给其他线程。



# 2. 锁优化

参考资料：[浅谈偏向锁、轻量级锁、重量级锁](https://www.jianshu.com/p/36eedeb3f912) 

## 2.1 自旋锁、适应性自旋

自旋锁和适应性自旋，都是对互斥同步（悲观锁）（synchronized、ReentrantLock都适用）的一种优化方式。

**自旋锁**

由于互斥同步在无法获取到锁时，线程会进入阻塞状态，而阻塞需要操作系统进入内核态来完成，开销较大。而许多应用上，共享数据的锁定状态只会持续很短一段时间，为了这段时间去挂起和恢复线程不值得。

自旋锁做的事情是：在阻塞线程之前，尝试一定次数获取锁的操作，如果一定次数（默认10次）后，仍然无法获得锁，再由OS帮忙进入阻塞状态。

自旋锁的意义是，减少了线程的阻塞、唤醒的次数，将那些“即将获取到锁的线程”阻塞的性能消耗，转移给了CPU，提高了性能。

**适应性自旋**

jdk1.6以后，引入了适应性自旋。是指更聪明地决定自旋次数，而不是简单粗暴的默认10次。

如果在同一个锁对象上，自旋等待刚刚成功获得过锁，且持有锁的线程正在运行中，那么JVM会认为这次自旋很有可能再次成功，因此允许它自旋次数更多一些，如100次。

若一个锁对象很少通过自旋成功被获得过，那么就会减少或省略掉自旋过程，以避免浪费CPU资源。

## 2.2 锁消除

锁消除是指，对于一些代码上要求同步，但被检测到不可能存在共享数据竞争的锁进行消除。如果检测到一段代码，堆内存的所有数据都不会被其他线程访问到，那就可以把它们当作栈上的数据对待，认为它们是线程私有的， 同步加锁自然就无须进行。

这项优化消除的锁，一般都不是程序员自己加上的，而是JVM对代码编译之后不小心加上的锁。也就是，JVM先优化了一波代码，不小心加上了锁，再使用锁消除，来把不必要的锁去掉。

举个例子：

```java
// 编译前，看起来没有同步的代码
public String concat(String s1, String s2, String s3) {
    return s1 + s2 + s3;
}

// 编译后，为了避免频繁生成新的String对象，优化为StringBuffer
public String concat(String s1, String s2, String s3) {
    StringBuffer sb = new StringBuffer();
    sb.append(s1);
    sb.append(s2);
    sb.append(s3);
    return sb.toString();
}
```

经过锁消除后，虽然在StringBuffer中仍然有加锁的逻辑，但实际上并不会真正加锁。

## 2.3 锁粗化

对于一系列连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥操作也会导致不必要的性能损耗。

具体的粗化方式是，把循环体内的加锁解锁逻辑，移动到循环体外部，这样执行整个循环，只需要加锁解锁一次就可以了。

## 2.4 轻量级锁

轻量级锁的目标是，减少无实际竞争的情况下，使用重量级锁产生的性能消耗，包括系统调用引起的内核态与用户态切换、线程阻塞造成的线程切换等。

轻量级锁使用CAS操作来代替互斥量的操作。若CAS操作成功，则轻量级锁获取成功；否则说明已经有线程获得了轻量级锁，说明发生了锁竞争，不适合继续使用轻量级锁，接下来膨胀为重量级锁，即互斥量。

缺点：若锁竞争激烈，那么轻量级锁会很快膨胀为重量级锁，那么维持轻量级锁的过程就成了浪费。

## 2.5 偏向锁

在轻量级锁的基础上继续优化：如果不仅没有实际竞争，而且自始至终，使用锁的线程只有一个，那么维护轻量级锁都是浪费的。因为每次获取轻量级锁，都要至少执行一次CAS操作。但偏向锁的目标是，在无竞争、只有一个线程使用锁的情况下，减少使用轻量级锁产生的性能消耗。

偏向锁只有初始化时需要一次CAS操作，会在对象头信息中记录这个线程标志owner。而当同一个线程再次获取锁时，不需要再次执行CAS操作，可以直接获取到锁。若其他线程想要获取这个锁，发现自己并不是owner，那么偏向锁就会膨胀为轻量级锁。

偏向锁的名字可以理解为，当第一个线程进来时，偏向锁就偏心地认为，这个线程就是唯一要获取这个锁的线程了。

## 2.6 逃逸分析简介

2.2节介绍的锁消除优化，会检测不可能被多线程访问的对象，然后把锁消除。这里使用的检测方法就是逃逸分析。

逃逸分析的基本行为就是分析对象的作用域。在一个方法中创建了对象，若这个指针作为参数传递到其他方法中，称为**方法逃逸**；若外部线程也能拿到这个指针，则称为**线程逃逸**。

如果能证明一个对象不会逃逸到方法或线程之外，就可以为这个变量进行一些优化：

* **栈上分配**：若一个对象不会产生方法逃逸，则可以将这个对象直接创建在栈内存的方法栈帧上。这样，方法结束，栈帧出栈，对象直接销毁。避免了创建在堆内存上触发GC的消耗；
* **锁消除**：若一个对象不会产生线程逃逸，则可以将这个对象的同步措施（加锁解锁）消除掉。好处是，减少锁操作的次数，以及获取不到锁时，操作系统进入内核态将线程阻塞所产生的性能消耗。当然对于这种情况，会有自旋锁和适应性自旋来优化；
* **标量替换**：标量是指八种基本类型以及reference类型的变量；聚合量是指由标量组合而成的变量，Java中的对象就是一种聚合量。若一个对象不会发生逃逸，则可以把这个对象（聚合量）拆散，不直接在堆内存上创建完整的对象，而是让对象的成员变量分配在栈上（栈上存储的数据，有很大概率会被JVM分配至物理机器的高速寄存器中存储）。

## 2.7 总结

锁的分类，从开销最小到开销最大，分别是：

1. 偏向锁（假定从始至终只有一个线程获取锁）；
2. 轻量级锁（假定竞争不激烈，使用CAS操作就可以）；
3. 重量级锁（互斥锁，需要OS切换内核态）；

当一个线程首次尝试利用synchronized获取锁时，会首先从偏向锁开始，当不满足偏向锁的假定时，会膨胀为轻量级锁；当仍然不满足轻量级锁的假定时，会膨胀为重量级锁。



对锁的优化方式：

* 自旋；
* 适应性自旋；
* 消除；
* 粗化。



# 3. CPU的并发问题及解决

关于并发，较好的学习方式是把硬件（CPU）级别的并发、和JVM的多线程并发结合来看。物理机和虚拟机在处理并发问题中，有很多相似的解决方案。

总体来看，有以下几点是相似的：

- 缓存一致性问题
- 重排序优化问题



## 3.1 缓存一致性问题

### 3.1.1 问题来源

CPU在运行时一定会与内存交互，如读取运算数据、存储运算结果等，这个I/O操作是很难消除的。由于内存读取速度与CPU运算速度之间有几个数量级的差距，为了不让I/O操作成为瓶颈，计算机在CPU和低速内存之间加了多级缓存（寄存器、l1、l2、l3 cache）：将运算需要使用到的数据复制到缓存中，让运算能快速运行，当运算结束后再从缓存同步回内存中，这样CPU就无须等待内存读写了。

CPU速度与常见操作的速度对比：假设CPU执行一条指令需要1秒，那么：

* 一级缓存读取：1.3秒；
* 互斥锁的加锁和解锁：65秒；
* 内存寻址：260秒；
* CPU上下文切换（即线程、进程的切换）：65分钟；
* SSD硬盘寻址：4.5天；
* 调用网络服务（http、RPC等，0.5ms）：15天；
* 磁盘寻址（10ms）：10个月。

参考资料：[让 CPU 告诉你硬盘和网络到底有多慢](https://cizixs.com/2017/01/03/how-slow-is-disk-and-network/) 

下图是硬件级别的内存模型：

![a](https://i.loli.net/2019/08/29/s4fbVYj3pHnJlK2.png)



这样就会产生缓存一致性的问题：多个CPU同时把共享内存中的值复制到自己的缓存中，可能导致各自的缓存数据不一致，那么同步回主内存时，就无法确定以哪个CPU缓存中的值为准。



### 3.1.2 MESI协议

为了解决这个问题，操作系统在高速缓存和主内存之间加了一道“缓存一致性协议”，来保证多CPU同时操作一块共享内存时，不会产生数据不一致问题。

拿MESI协议来说明问题，它是这样解决多CPU之间缓存不可见问题的：

1. Core0修改变量v后，发送一个信号，将Core1缓存的变量v标记为失效，并将修改值写回内存；
2. Core0可能会多次修改变量v，每次修改都只发送一个信号（发信号时会锁住缓存间的总线），Core1缓存的变量v保持着失效标记；
3. Core1使用变量v前，发现缓存中的v已经失效了，得知v已经被修改了，于是重新从其他缓存或内存中加载。

以上即是MESI（Modified Exclusive Shared Or Invalid，缓存的四种状态）协议的基本原理，不算严谨，但对于理解缓存可见性（更常见的称呼是“缓存一致性”）已经足够。



## 3.2 指令重排序问题

### 3.2.1 问题来源

为了使CPU内部的运算单元尽量被充分利用，CPU可能会对输入的指令进行**乱序执行**优化，并在计算之后，将乱序执行的结果进行重组，保证该结果与顺序执行的结果是一致的。

问题来了：由于只能保证最终结果的一致性，并不保证代码逻辑的先后顺序与实际执行的先后顺序一致，因此，如果存在一个计算任务依赖于另外一个计算任务的中间结果，那么顺序性不能靠代码逻辑的先后顺序保证。

### 3.2.2 指令重排的原则

CPU是如何解决这件事的呢？CPU在进行指令重排时，必须保证：指令上下文的因果关系、依赖关系不发生改变。换句话说，只要不影响程序单线程、顺序执行的结果，就可以对两个指令重排序。举例：

```java
a++; b=a*2; c--;
```

可能会被重排为：

```java
a++; c--; b=a*2;
```

但一定不会出现：

```java
b=a*2; a++; c--;
```

因为对b的赋值操作依赖于a的计算结果，这样优化后改变了a和b的依赖关系。

Java虚拟机的即时编译器中，也有类似的指令重排序优化。



# 4. JVM的并发问题及解决

## 4.1 Java内存模型

对于Java，也有自己的内存模型（JMM，Java Memory Model）。这套关于并发的内存模型是一个逻辑模型，而之前讲的堆、栈、方法区等等是JVM在物理内存上的真实划分。稍后会说明这两套模型的对应关系。

JVM内存模型规定了，所有变量存储在**主内存**（类比上图的主内存），每条线程还有自己的**工作内存**（类比CPU高速缓存）。

线程工作内存中，保存了该线程操作的变量在主内存中的拷贝，线程对变量的所有操作（读取、赋值）必须先在工作内存中完成，然后再同步到主内存中去。线程不能直接读写主内存中的变量。

> 关于从主内存到工作内存的拷贝，对于一个大对象，只会拷贝当前线程需要操作的某一个属性，不会拷贝整个对象。原则是尽可能少的向工作内存中拷贝。

下图是JVM的内存模型，可以与上图对比：

![a](https://i.loli.net/2019/08/29/HrxcPYpkVSTvqbO.png)



上图与JVM内存的物理划分的对应关系是：

* 主内存：对应堆内存，准确说是其中存储数据的部分；

* 工作内存：对应栈内存的部分区域，但为了获得更好的运行速度，JVM会让工作内存优先存储于寄存器和CPU高速缓存中，因为程序运行时主要访问读写的是工作内存。

因此上图是一个逻辑模型，我们其实不需要关心主内存和工作内存到底存放在什么位置，只需要分析这种模型产生的线程安全问题，以及如何解决它们。

按照这样的内存模型，就引发了线程安全问题（请与CPU缓存不一致问题类比）：多个线程同时操作一个对象，都拷贝到自己的工作内存中，无法保证工作内存中的数据一致性。若两个线程基于同样的拷贝对象进行操作，则先执行完的线程结果，一定会被后执行完的线程结果所覆盖。这就是常说的线程安全问题。

此外，多个线程都进行代码重排序，也会产生线程安全问题。



## 4.2 8种原子性操作

Java如何解决线程安全问题？JMM提供了8种原子性操作，我们熟悉的锁相关的关键字（volatile、synchronized），底层都是利用这8种原子性操作来完成的。如下图所示：

![36c022bed418ad653a2f22f15d2b468.jpg](https://i.loli.net/2019/08/29/tyK6Yb4DguSWEkw.jpg)

* lock（锁定）：作用于主内存对象，将对象监视器从0变为1，表示对象处于内存独占状态；
* unlock（解锁）：作用于主内存对象，将对象监视器从1变为0，表示对象无锁；
* read（读取）：作用于主内存对象，把主内存对象传输到工作内存中，以便load使用；
* load（载入）：作用于工作内存对象，工作内存中的变量指针，指向刚刚read到的内存区域；
* use（使用）：作用于工作内存对象，线程使用对象时，将对象的值传递给执行引擎；
* assign（赋值）：作用于工作内存对象，线程使用完对象，将计算后的新值赋给工作内存中的副本对象；
* store（存储）：作用于工作内存对象，把工作内存对象传输到主内存中，以便write使用；
* write（写入）：作用于主内存对象，主内存中的变量指针，指向刚刚store的内存区域。



## 4.3 先行发生原则

面对无数种多线程的实际场景，我们如何证明一段代码是线程安全的？

Java内存模型提供了八种天然的先行发生关系。这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，JVM可以对它们随意进行重排序。

也就是说，我们要证明一段代码是否线程安全，就要看每一对需要保证先后顺序的操作是否真的具有先行发生的关系。如果存在那么一对操作，调用顺序是A -> B，它们需要保证A的结束一定发生在B的开始之前，但无法从以下规则中推导出来，则说明这段代码存在线程安全的风险。

先行发生（happens before）原则如下：

* 程序次序规则：**写在前面的（或在循环、分支等逻辑靠前的）代码**一定先行发生；

  * 这就保证了在所有单线程环境下，不会由代码重排序引发安全问题；

* 管程锁定规则：**同一个锁对象的unlock操作**，一定先行发生于**下一次lock操作**（这里lock和unlock在代码上使用synchronized实现）；

  * 管程是对PV操作的封装，屏蔽了PV操作编写复杂和易错的缺点，在Java中就是synchronzied；

* volatile变量规则：**对一个volatile变量的写操作**，一定先行发生于**下一次读操作**；

  * 注意，这条规则保证了volatile具有可见性。
  * 对一个普通变量先写后读（两个线程分别做），写操作还没有完成，读操作会读到原来的值。这就会引发“不可见”的问题。volatile通过这条规则避免了不可见。

* 线程启动规则：**线程start()方法**一定先行发生于**此线程的每一个操作**；

* 线程终止规则：**线程的每一个操作**都一定先行发生于**对此线程的终止检测**；

* 线程中断规则：若线程被调用**interrupt()方法**，则此方法一定先行发生于**此线程的代码检测到中断事件的时刻**；

  * 啥意思呢，举个例子：

    ```java
    线程A：
    threadB.interrupt(); // (1)中断线程
    
    线程B：
    public void run() {
    	while(!Thread.interrupted()) { // (2)检测线程中断
    	}
    }
    ```

    在(2)语句返回true之前，一定是(1)先行发生。

* 对象终结规则：一个对象的**构造方法执行结束**，一定先行发生于**finalize()方法的开始**；

* 传递性：若操作A先行发生于操作B，操作B先行发生于操作C，就可以得出：操作A先行发生于操作C。



**注：对先行发生的定义**

“先行发生”与“时间上的先发生”不同。

“时间上的先发生”是指“时间顺序上优先被调用”，A先开始执行，B后开始执行，但可能A还没有执行完，B就开始执行了，如果B的执行依赖于A的结果，就会产生问题。

“先行发生”则是指，A先执行，等到A已经执行完了，才允许B开始执行。这叫A先行发生于B。这就不怕B依赖于A的结果了，反正A都执行完了。



## 4.4 volatile

当一个变量被定义为volatile之后，它具备两种特性：

* 保证此变量对所有线程的**可见性**；
* **禁止指令重排序**优化。

volatile保证了对变量的读、写操作的原子性（读写操作无法在一个没执行完的情况下就执行下一个），但是并不保证对变量的所有操作的原子性。比如对变量的递增（i++），这个操作就不是原子性的。因此这时使用volatile也会有线程安全问题。

### 4.4.1 可见性

volatile如何保证可见性？对于一个volatile变量：

* 被更新后，立即执行：store、write，将工作线程的副本立刻同步进主内存，同时使其他工作线程的副本失效；
* 被读取前，必须执行：read、load，将主内存的数据重新拷贝到线程自己的副本；
* volatile的先行发生原则：对一个volatile变量的写操作，一定先行发生于下一次读操作。

试想，如果只保证了前两条，仍然可能产生问题。想象这样一种情况：线程A先开始写入一个变量，还未写入完成时，B开始读。这时B读到的一定是旧数据，无法保证可见性。

![f05ce56540dde70da260dbb96c4379d.jpg](https://i.loli.net/2019/08/29/ajAU7TFCEIb9Gvi.jpg)



因此，先行发生原则中，又增加了一条“volatile变量规则”，对于一个volatile变量，必须写完了才允许读。



从8种原子性操作的角度来理解，volatile关键字的规则也可以描述成：

- read、load、use动作必须**连续出现**；
- assign、store、write动作必须**连续出现**。

这与上面的定义是一样的。

### 4.4.2 有序性

volatile利用一条内存屏障的指令来实现禁止重排序。上节讲的对volatile变量赋值流程如下：

```
（1）对变量赋值
（2）【内存屏障】执行store、write操作
```

由于store操作是将线程私有内存中的数据copy到主内存中去，因此这条操作一定依赖于赋值的结果——也就是说，（1）和（2）操作是有依赖关系的，（1）一定先行发生于（2）。

因此，执行到（2）语句时，意味着之前的赋值操作一定已经完成，这样便形成了“指令重排序无法越过内存屏障”的效果。

### 4.4.3 适用场景

由于volatile只能保证可见性和有序性，无法保证对变量多条操作的原子性（比如线程不安全的 `i++` ），因此相比于锁而言，它的适用场景稍稍偏窄。只有在满足如下两条时，才可以使用volatile：

1. 运算结果并不依赖变量当前的值，或者能够确保只有单一的线程修改变量的值；
   * 理解：因为如果依赖当前值的话，先计算再赋值，在没有其他锁的情况下，一定会有线程安全问题；
2. 该变量不需要与其他的变量共同纳入不变性条件中；
3. 在访问变量时不需要加锁。

举例：

```java
private static volatile boolean jump = false;

new Thread(() -> {
  while (!jump) {
  }
  System.out.println("jump out!");
}).start();

Thread.sleep(50);
jump = true;
```

分析：若jump变量没有volatile修饰，则最后的 `jump = true;` 语句将没有效果——对子线程不可见。因为内部线程早已经把jump的值拷贝到自己的私有内存中去了，每次循环都会命中私有内存，当然不会费事去主内存中读取。

jump加上volatile之后，执行 `jump = true;` 后，会立刻同步到主内存上去，并且把子线程中的jump值标记为失效。这样子下次下次读取时，发现本地内存中的jump值失效，则会去主内存中读取，就能读到修改后的值了。

## 4.5 synchronized

synchronized是重量级锁（还有很多其他称呼：悲观锁、互斥锁），volatile支持的特性它当然也会支持。除了可见性和禁止重排序外，还能保证操作的原子性。

### 4.5.1 原子性

通过加锁和解锁的方式实现原子性，即JVM提供的lock和unlock操作。

### 4.5.2 可见性

得到锁时，会从内存里重新读入数据（read、load），刷新缓存；

释放锁时，会将数据回写到内存（store、write），以保证对其他线程可见。

### 4.5.3 有序性

由于synchronized块内的代码一定是单线程执行，再结合先行发生原则中的“程序次序规则”，因此一定能保证有序性。



## 4.6 总结

* 缓存一致性问题：
  * CPU硬件层面，使用MESI协议解决；
  * Java内存模型层面，使用volatile解决；（在JMM中称为“可见性”）
* 重排序问题：
  * CPU硬件层面，CPU指令重排序时，分析语句间的依赖关系来保证结果正确；
  * Java内存模型层面，进行编译器重排序，volatile使用内存屏障来避免重排序问题。

| 关键字       | 原子性  |    可见性     |    有序性     |
| :----------- | :-----: | :-----------: | :-----------: |
| synchronzied | √（锁） | √（缓存失效） |  √（单线程）  |
| volatile     |         | √（缓存失效） | √（内存屏障） |





# 5. 设计一个线程安全的类

> 《Java并发编程实战》第四、五章

要设计一个线程安全的结构，可以按以下步骤：

1. 找出构成对象状态的所有变量；
2. 找出变量的约束条件；
3. 建立对象状态的并发访问管理策略；

第一点很容易理解，对象的状态由内部变量组成，可以形成一个树结构，当前对象（根节点）包含一堆成员变量（直接子节点），每个成员变量如果是基本类型，它就是叶子节点；成员变量如果仍然是对象，那么它可以有自己的子节点。

第二、三点下面分别讲解。

最后要注意的是，在设计一个并发数据结构时，一定在文档上清晰的说明，这个类是否是线程安全的、类通过哪些手段（锁、volatile等）实现了线程安全。因为这种数据结构很容易过两天就忘记具体实现了。

## 5.1 变量的约束条件

**不可变条件**

即静态地判断一个变量是否有效。例：

- 集合结构中的size，有效范围一般是0到Integer.MAX_VALUE；

**先验条件**

对一个变量的修改，必须依赖于其他变量的状态。

单线程中程序中，如果某个操作无法满足先验条件，那就只能失败。但在并发程序中，先验条件会由于其他线程执行的操作而变成真。并发程序中要一直等到先验条件为真，然后再执行该操作。这个“等待”，就需要使用生产者消费者模型了，Java中可以用Object类的wait和notify来实现，也可以使用ReentrantLock#Condition的await和signal来实现。例：

- 队列中，必须size>0才能执行弹出操作；
- 阻塞队列中，size==0时，take操作必须等待另外一个put操作执行后才能执行；

**后验条件**

判断变量的改变是否有效。对于这种后一个状态依赖于前一个状态的操作，必须使用同步机制来保证原子性。例：

- 递增操作，前一个状态是n，那么后一个有效状态必须是n+1；

**同时约束多个变量的条件**

多个相关的变量，必须在单个原子操作中进行读取或更新。例：

- 一个对象中包含两个变量：上界和下界，那么必须保证上界大于等于下界；
- 线程安全的集合结构中，size的数值必须与集合元素数量时刻保持一致；

## 5.2 并发访问管理策略

### 5.2.1 实例封闭

通过private+锁方法的方式，让变量在同一时刻只能由同一个线程访问。

private保证对象只能被本类的方法访问到，不会逃逸到其他方法上；锁方法保证只有获取当前实例锁的线程才能访问。

例：Collections#synchronizedList方法，使用private+锁对象的方式实现线程安全的list。

【插播】此方法用到了装饰器模式；

### 5.2.2 委托给线程安全的类

将线程安全问题，交给已经定义好的线程安全类去解决。jdk中常用的线程安全的类都在java.util.concurrent包中。

前三种（map、list、队列），是jdk提供的线程安全数据结构；后三种（闭锁、信号量、栅栏），是jdk提供的同步工具类。

#### 5.2.2.1 map结构

常用的是ConcurrentHashMap。它使用更细粒度的分段锁机制。读读之间不互斥，对于多线程同时读写、或同时写，只要没有命中同一个segment段，就不是互斥操作。它的优势是，在并发环境下实现了更高的吞吐量，而在单线程环境中只损失了很小的性能；

#### 5.2.2.2 list结构

**CopyOnWriteArrayList**

“copy on write”指的是，集合中的内容默认是不会改变的，这样就会是线程安全的；而一旦发生改变了怎么办？copy一份副本即可。因此，看源码会发现，add、remove等修改方法中，都是通过数组复制实现的。

问题来了，CopyOnWriteArrayList的写操作已经加锁了，为什么还需要复制数组呢？在锁内像ArrayList一样新增、扩容不好吗？这其实是为了方便遍历考虑的。遍历过程中，一定不能进行写操作，否则会导致遍历不全或越界。因此，对于每一次遍历（一次 `iterator()` 方法调用），都会创建一个快照，单独给这次遍历使用。而这样也就保证了遍历过程中可以同时写，因为遍历的是快照，写的是本体，互不冲突。

因此，此结构适用于遍历操作远多于写操作的场景，毕竟元素较多时复制整个数组，是一个很耗时的操作；

**Collections#synchronizedList**

同样是线程安全的list结构，只是给读写操作都加了锁。因此适用于写操作更多的场景。

注意，这两种虽然都是线程安全的list结构，但CopyOnWriteArrayList的复制数组，是为了并发遍历而设计的结构，而不是多线程同时写。而并发情况下很少有get(index)这样的操作，一般都是在队头队尾的操作，这样就把场景限制在队列结构中。因此没有提供纯读写锁的list。

如果需要实现线程安全的队列，可以用下面的无界阻塞队列来实现。

#### 5.2.2.3 队列结构

线程安全的队列是阻塞队列，核心接口是BlockingQueue。可以利用阻塞队列来实现生产者-消费者模式。jdk自带的线程池（ThreadPoolExecutor）就是利用阻塞队列，对生产者-消费者模式的一种实现。

* LinkedBlockingQueue和ArrayBlockingQueue：是使用链表和数组实现的队列结构，只支持put（队尾入队）和take（队头出队）的操作；
* LinkedBlockingDeque：双端队列结构（队列+栈），支持队头、队尾分别入队、出队，共4个操作。可用于实现工作窃取（密取）的任务执行框架，一个线程执行完了，去其他线程的队列末尾偷点任务来执行。为啥一定要从队尾去偷呢，是为了降低同时从队头取元素产生的并发性能问题；
* PriorityBlockingQueue：使用优先队列实现的阻塞队列，支持元素按自定义的优先级排序；
* SynchronousQueue：一种不维护元素的队列结构，相反，它维护着两组线程：生产者线程和消费者线程，控制它们的阻塞与唤醒。当生产者生产了一个元素后，它会直接将元素交给一个阻塞中的消费者，省去了入队列的步骤。

#### 5.2.2.4 闭锁结构

闭锁的作用是，当多线程并行的执行某个任务时，等最慢的那个执行完，再继续执行下面的任务。

* CountDownLatch：初始化时设置一个等待的线程数量，每个线程执行完，都调用一下 `countDown()` 方法。最后在主线程中，调用 `await()` 方法，等待最慢的那个线程执行完，主线程再继续执行；
* FutureTask：对于一个任务，它提供了一个 `get()` 方法，来异步地获取任务结果。若任务已完成，则立即返回，否则将阻塞直到任务完成。使用线程池+CountDownLatch可以达到相似的效果。

#### 5.2.2.5 信号量结构

Java中的信号量结构Semaphore，是jdk对操作系统中的信号量概念的一种实现。可用于空置同时访问某个临界资源的操作数量。内部使用 `acquire()` 和 `release()` 分别表示资源的消费与生产（信号量的PV操作）。

信号量的用途：互斥锁；读写锁；生产者-消费者模式等等。

TODO 看源码，如何用Java实现信号量机制

#### 5.2.2.6 栅栏结构

会让所有线程都等待完成后，才会继续下一步行动。

举个例子，就像生活中我们会约朋友们到某个餐厅一起吃饭，有些朋友可能会早到，有些朋友可能会晚到，但是这个餐厅规定必须等到所有人到齐之后才会让我们进去。这里的朋友们就是各个线程，餐厅就是栅栏。

**CyclicBarrier**

循环栅栏，即可重复利用的栅栏。初始化时传入两个参数：参与线程的个数parties，和所有线程到达栅栏后执行的任务barrierAction（该参数可选）。每个线程通过调用await()方法表示自己已经到达栅栏，await()之后的代码表示，所有线程都到达栅栏之后要执行的逻辑。

适用于多线程计算数据，最后合并计算结果的场景，与CountDownLatch的区别在于：

* CountDownLatch 是一次性的，CyclicBarrier 是可循环利用的；
* CountDownLatch 参与的线程的职责是不一样的，主线程在等待倒计时结束，其他子线程都在倒计时。CyclicBarrier 参与的线程职责是一样的，不分子/主线程。

**Exchanger**

一种两方（Two-party）栅栏。用于两个工作线程之间交换数据的栅栏结构，我们不确定两个线程哪个先走完，所以第一个走完的线程会等待另一个，当都执行结束后，唤醒第一个，进行交换数据。

#### 5.2.2.7 显式锁结构

**ReentrantLock**

是一种支持响应中断、公平锁、线程分组阻塞唤醒的互斥锁结构。详细讲解见1.3.1.3节。

**ReadWriteLock**

是jdk提供的读写锁结构，支持读写互斥、写写互斥、读读不互斥。具体实现类是ReentrantReadWriteLock。其中一些可配置项包括：

* 释放优先：一个写锁被释放时，应该优先唤醒阻塞中的读线程、写线程，还是最先发出请求的线程？
* 读线程插队：假设临界资源正在被读，但有写线程正在等待，那么新到达的读线程能否立即获得访问权，还是应该在写线程后等待？若允许读线程插队到写线程之前，那么将提高并发性，但可能造成写线程饥饿的问题；
* 重入性：读写锁是否可重入？
* 降级：如果一个线程持有写锁，那么它能否在不释放锁的情况下获得读锁？锁降级的意义在于，写线程写完后立刻读，能保证读到自己刚刚写入的结果，而不会由于线程安全问题读到别人改过的结果；
* 升级：这是不支持的，即读锁升级为写锁。

例：利用ReentrantReadWriteLock，包装一个支持读写锁的Map结构。代码见com.learning.project.fzk.lock.ReadWriteMap。

## 5.3 本地缓存工具设计

本节的目标是，设计一个线程安全的结果集缓存结构，其中存储K-V结构的数据。下面每一节都会在上一节的基础上修复一些问题，最终改进为一个优秀的本地缓存工具类。

### 5.3.1 ConcurrentHashMap

首先，由于是K-V结构，首先想到的是map，而HashMap线程不安全，因此可以使用ConcurrentHashMap：

```java
private ConcurrentHashMap<K, V> map = new ConcurrentHashMap();

public V getResult(K key) {
    V value = map.get(key);
    // 命中缓存，直接返回
    if(value != null) {
        return value;
    }
    // 未命中，计算并放入缓存
    value = calculate(key);
    map.put(key, value);
    return value;
}
```

注，这里假设 `calculate(key)` 是直接计算的逻辑。

这种方案在 `calculate(key)` 方法很快时问题不大，但如果该方法耗时较长，会产生一种情况：多线程同时请求同一个缓存中不存在的key，这时，多线程就会同时进入计算逻辑。这样会导致不必要的重复计算，且导致CPU占用率飙升。

### 5.3.2 使用Future

要改进上面的问题，比较好的思路是：在开始计算前，向map中放入一个占位符，表明我这个线程开始计算了，其他线程稍等一会，我算完之后放到map里你们再去拿。这样，其他线程看到占位符，可以进入阻塞状态，并等着被唤醒就好了。

阻塞和唤醒的逻辑可以自己用 `wait()` 和 `notifyAll()` 来实现，也可以利用现成的 `FutureTask` 类来实现。下面是使用 `FutureTask` 实现的版本：

```java
private ConcurrentHashMap<K, Future<V>> map = new ConcurrentHashMap();

public V getResult(K key) {
    Future<V> future = map.get(key);
    // 未命中，则向缓存中插入Future占位符，并开始计算
    if(future == null) {
        FutureTask<V> task = new FutureTask<V>(() -> calculate(key));
        map.put(key, task);
        task.run(); // 开始计算
    }
    // 此时，保证要么当前线程正在计算中，要么其他线程已经计算结束
    // 从中拿到结果（阻塞一会/立即拿到）后返回
    return future.get();
}
```

这种方式保证了，即使计算任务比较耗时，也不会引发重复计算的浪费问题。

### 5.3.3 CAS、处理异常

但逻辑上仍然存在两个漏洞：

1. **仍然不是绝对的线程安全**。 `if(future == null)` 这个判断，和 `map.put(key, task);` 这个操作，本应该保持原子性的。也就是说，即使ConcurrentHashMap本身是线程安全的，但它只能保证对它的单一方法调用是原子性的，无法保证多个方法的组合调用的原子性。

   解决方法是，使用ConcurrentHashMap提供的原子性”先检查再执行“操作： `putIfAbsent(K key, V value)` 。可以看做是一个CAS操作，把检查和执行放在一个原子性操作中。

2. **没有处理计算失败的情况**。当 `calculate(key)` 方法失败时，map中永远保存的是那个没有计算完的Future占位符，导致获取该key的线程永远阻塞。

   解决方法是，若计算失败，将map中的Future占位符移除，并重新开始新一次的计算。

改进版本如下：

```java
private final ConcurrentHashMap<K, Future<V>> map = new ConcurrentHashMap();
private final Computable<K, V> computable;

public V getResult(K key) {
    while (true) { // 加一层循环，处理计算失败的情况
        Future<V> future = map.get(key);
        // 未命中，第一次检查
        if (future == null) {
            FutureTask<V> futureTask = new FutureTask<>(() -> computable.compute(key));
            // 第二次原子性检查，并记录是否put成功：
            // 若返回值为空，说明成功抢到锁并put进去；若返回值非空，说明已经有其他线程抢先put进去了
            future = map.putIfAbsent(key, futureTask);
            if (future == null) { // 只有是当前线程put成功，才开始计算，否则等着其他线程算完就行了
                future = futureTask;
                futureTask.run(); // 开始计算
            }
        }
        // 此时，这里的future可能是当前线程put进去的，也可能是当前线程put失败，返回了其他线程的。
        // 从中拿到结果（阻塞一会/立即拿到）后返回即可
        try {
            return future.get();
        } catch (Throwable e) {
            // 若compute抛出异常，移出future重新计算
            map.remove(key, future);
        }
    }
}
```

源代码见：com.learning.project.fzk.multiThread.cache.Cache2

测试代码见：com.learning.project.fzk.multiThread.cache.TestCache

注意：

1. 使用了双重检查，第一次检查确保Future占位符为空才创建新的Future，不会每个线程都创建一个，但这与后面的put方法组合后，是线程不安全的；因此需要第二次检查，这次检查是原子性的，保证只有一个线程能执行put方法成功。跟双重检查加锁的单例模式思路一样，这里也通过双重检查加锁的方式保证了运算过程和结果的单例；
2. map结构的 `put` 、 `putIfAbsent` 、 `computeIfAbsent` 等方法，返回值都是方法执行之前，key对应的value值。也就是说，如果map本身不包含一个key，这些方法返回的就是null。

运行流程：

* 100个线程同时获取同一个key的结果；
* 10个线程第一次判断通过，于是创建了10个Future占位符；剩下90个都在future.get()方法中阻塞了；
* 10个线程同时对自己的Future占位符执行putIfAbsent操作；
* 只有1个线程执行成功，并返回null，于是自己执行；
* 其他9个线程执行失败，并返回上面成功的线程创建的Future占位符，同样在future.get()方法中阻塞；
* 若唯一成功的Future执行失败，则阻塞中的99个线程都会捕获异常，并进行重试；
* 直到某一次的Future执行成功，则唤醒99个线程获取结果。

### 5.3.4 computeIfAbsent

另外，jdk1.8版本后，ConcurrentHashMap中提供了原子性的“若不存在则开始计算”的操作： `computeIfAbsent(K key, Function<? super K, ? extends V> mappingFunction)` ，传入一个计算的函数，若key不存在，则调用计算函数进行计算，并放入map结构中。由于整个操作是原子性的，因此不必担心多线程同时执行计算函数：只有第一个线程会执行，其他的都会暂时阻塞。

与上节putIfAbsent的唯一区别是，putIfAbsent的入参 `FutureTask#Callable#call` 方法，支持抛出异常；而computeIfAbsent的入参Function的函数apply，不支持抛出异常，因此循环计算的逻辑要移到map里面去。

因此，可以使用 `computeIfAbsent` 函数来替代上面的 `Future` ，如下：

```java
private ConcurrentHashMap<K, V> map = new ConcurrentHashMap();

public V getResult(K key) {
    while (true) { // 加一层循环，处理计算失败的情况
        V v = map.get(key);
        // 未命中，第一次检查
        if (v == null) {
            // 第二次原子性检查
            v = map.computeIfAbsent(key, k -> {
                while (true) { // 处理计算失败的情况
                    try {
                        return computable.compute(key);
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            });
        }
        return v;
    }
}
```



# 6. 线程池

> 《Java并发编程实战》第六、七、八章

当大量任务同时到达服务器时，服务器需要一种策略，来保证高吞吐量、响应迅速地执行这些任务。

我们不能用单线程串行的方式去执行，因为每次只能执行一个请求，吞吐量太低。若任务大多是I/O密集型的话，没有很好的利用CPU资源。

我们也不能给每一个任务单独起一个线程，这样会导致线程之间切换过于频繁，频繁切换用户态和内核态本身就是一种消耗。且Linux上每个线程默认占用1MB的栈内存，任务过多会产生OOM问题。

因此，我们需要一种线程复用的技术。这些线程当执行完一个任务后不会消亡，而是继续执行另一个任务。若暂时没有任务，可以让这个线程阻塞，直到新的任务进来。重用线程极大降低了线程创建和销毁过程的开销，且请求到达时，工作线程通常已经存在，因此不会由于等待创建线程而延迟任务的执行，提高了响应性。

这就是线程池，jdk提供的ThreadPoolExecutor是对线程池的一种实现。

## 6.1 线程池参数

**corePoolSize、maxPoolSize：核心线程数、最大线程数**

线程池会根据这两个参数自动调整池子的容量。当一个新任务提交给execute()方法时，运行机制如下：

1. 若当前线程数<核心线程数，会创建新线程来跑这个任务，即使其他线程正在闲置；
2. 若核心线程数<当前线程数<最大线程数，只有当队列满了时，才会创建新线程；
3. 如果core和max值设成一样，那就成了一个固定长度的线程池。max最大可设置为Integer.MAX_VALUE；

通常这两个参数通过构造方法来设置，但也可以通过set方法来动态改变。

**ThreadFactory：创建新线程**

新线程会通过ThreadFactory创建。默认使用Executors#defaultThreadFactory，创建出来的线程属于同一线程组，且优先级相同，非守护线程。
可自定义ThreadFactory，改变线程名字、线程组、优先级、是否守护线程等。
若一个ThreadFactory创建线程失败（返回null），线程池会继续运行，但可能无法执行任何任务。

**keepAliveTime：线程存活时间**

若线程数量多于core值，超额的线程会在它们闲置超过keepAliveTime时间后被终止。
这是一种在线程池不活跃使用时的减少资源消耗的方法。该参数可用set方法手动修改。
该事件默认只对core之外的线程起作用，但allowCoreThreadTimeOut()方法可以指定是否对core内的线程起作用（当然前提是这个时间值非0）。

**workQueue：存储任务的阻塞队列**

任意一个阻塞队列（BlockingQueue）都可以存储任务。队列的使用会影响线程池的长度。
新任务的优先级是：core > 队列 > max > 拒绝。

**handler：任务拒绝策略**

当队列已满、线程数量达到max时，再提交进来的任务会被拒绝。默认提供四种预定义的拒绝策略：

1. AbortPolicy：抛异常；
2. CallerRunsPolicy：直接调用run()方法去执行任务（相当于把异步任务变成了同步）。这会减慢新任务提交的速度；
3. DiscardPolicy：啥都不干，任务直接被抛弃（好惨）；
4. DiscardOldestPolicy：队列首个任务被抛弃，然后任务重试。

当然可以自定义拒绝策略。

**hook方法**
beforeExecute()、afterExecute()：在每个任务执行前后执行一些方法（类似于AOP）。

若一个线程池，程序不再引用、且没有剩余线程，会自动关闭。

## 6.2 最优参数配置

### 6.2.1 CPU密集型程序

**先推理**

首先，举个例子：多线程计算2-20000的全部素数。

可以确定，这是一个CPU（计算）密集型的程序，处理速度完全取决于CPU的运算速度，与外界因素无关。

假如CPU核数 = 1，那么无论单线程还是多线程，全部运算量都要怼到这个CPU上，速度相同。而多线程还会增加CPU切换线程上下文的消耗，反而更慢。

假如CPU核数 = 4，那么线程数设为4的性能最高。若小于4，则会有CPU处于空闲状态；若大于4，仍旧是所有CPU全速运行，直到停止，所以对程序性能并没有提升，反而会因为切换上下文对性能有所消耗。

![clip_image002.jpg](https://i.loli.net/2019/09/25/TJezsfAK7vPDgI3.jpg)

 

同时用4个线程来处理，那么一个CPU上的时间片分配，大概是这样：

![clip_image004.jpg](https://i.loli.net/2019/09/25/tSDyHz9ORaCdPpm.jpg)

 

结论：CPU（计算）密集型的程序，线程数设置为CPU核数即可。

补充：《Java并发编程实战》8.2节认为，CPU密集型程序，线程数可以设置为CPU核数+1，这样即使当线程偶尔由于页缺失故障或其他原因而暂停时，这个“额外”的线程也能确保CPU的时钟周期不会被浪费。

**实际效果**

下图是：8核处理器，多线程运行找素数程序的效果图。

![clip_image006.jpg](https://i.loli.net/2019/09/25/tlhCjO9bfAGYovk.jpg)

可以看出：

1. 不管如何划分任务，8核8线程，已经达到性能极限值，更高的线程数不会降低运行时间；
2. 线程数与运行时间成反比：4线程的运行时间是8线程的两倍，符合猜想。

### 6.2.2 IO密集型程序

**先推理**

这类程序运行时通常包括：计算+阻塞 两部分。

比如查询SQL（发出查询请求 + 等待SQL执行）、读写文件（把命令提交给OS + 等待OS执行）、web服务调用（发出http/RPC请求 + 等待返回结果）。

首先一个大前提：**当一个线程阻塞时，CPU会立刻终止线程所在的时间片，将运行权交给其他线程。**

再来一个小前提：IO密集型的程序，阻塞时间一般远大于计算时间。

于是，IO密集型程序多线程运行情况如下（下图没有时间片的概念，反正会被CPU提前终止）：

![clip_image008.jpg](https://i.loli.net/2019/09/25/Btv8N3cHSMkmpbs.jpg)

 

问题来了：从ThreadD第一次计算结束，到ThreadA第一次阻塞终止，中间这段时间CPU是空闲的。四个线程全部阻塞了！

所以，为了最大化利用CPU的计算资源，我们希望所有时刻CPU都是在计算的。于是能想到如下公式：
$$
线程数=\frac{计算时间+阻塞时间}{计算时间}
$$
这样就会保证，当最后一个线程刚刚发出查询SQL的请求，第一个线程恰好获取到SQL结果，阻塞停止。完美衔接。

完善一下，如果有多个CPU核心，则需要乘以CPU核心数量；如果不希望占用全部CPU资源，还要乘以期望CPU占用率。这样，就得到完整的公式：

Nthreads = Ncpu * Ucpu * (1 + W/C)

其中：

* Ncpu是处理器的核的数目
* Ucpu是期望的CPU利用率（该值应该介于0和1之间）
* W/C是等待时间与计算时间的比率

 

**实际效果**

下图的任务数据：2核机器，共40个任务。

![clip_image012.jpg](https://i.loli.net/2019/09/25/UFj6WdZJnpaDPh4.jpg)

 

线程数=20是一个分界点，20之前只要增加线程数，都会提高性能。20之后基本就是水平线了（因为没有那么多任务去跑，会有空闲线程）。

这种阻塞时间远大于计算时间的任务，就应该线程数越大越好，最好让所有任务一次性发出命令，一起阻塞住。这是最省时间的策略。

##  6.3 原理解析

TODO 画一张好看点的图...

![clip_image001.jpg](https://i.loli.net/2019/09/27/qowfZShQGuvpItl.jpg)

![clip_image003.jpg](https://i.loli.net/2019/09/27/cUO9HPkfamBjzeJ.jpg)





#  7. 死锁

> 《Java并发编程实战》第10章

## 7.1 死锁是什么

参考操作系统中的知识，死锁发生的四个必要条件是：（1）互斥（2）不可剥夺（3）持有并等待（4）循环等待。哲学家问题是一个经典的产生死锁的场景，只要破坏上面任意一个条件（通常是3或4），比如给资源编号，或者限制抢夺锁的条件，那么死锁问题就会解决。

数据库系统中也会存在死锁问题。在一个事务中可能需要获取多个锁，并一直持有这些锁，直到事务提交。因此在多线程环境下很可能发生死锁，多个事务将永远阻塞。但数据库系统不会让这种情况发生。数据库系统会构造一个“等待关系的有向图”，其中的循环结构就代表死锁。当它检测到一组事务发生了死锁时，将选择一个牺牲者放弃这个事务，这样其他事务就会继续执行，从而打破死锁。

【TODO 学习操作系统中的死锁检测算法】

但JVM中并没有做这种处理。当一组Java线程发生死锁时，这组线程真的会永远阻塞。因此需要通过程序分析的方法来避免死锁。

## 7.2 死锁场景

### 7.2.1 典型的死锁

两个线程分别抢夺两个资源，而这两个线程抢夺资源的顺序却相反。

```
ThreadA       ThreadB
lock(m)       lock(n)
     此处产生死锁
lock(n)       lock(m)
```

解决方法可以套用哲学家问题，或者想办法打破任意一个死锁的必要条件。如：让所有线程以固定的顺序来获取锁。即，线程A和B都以先m后n的顺序获取锁，这样能保证一定有一个线程同时获取到m和n，等这个线程执行完，另一个线程再开始执行。



### 7.2.2 互相转账问题

互相转账问题，是一种不那么容易发现的死锁。假设一个转账逻辑如下：

```java
// 入参：操作人、收款人、转款金额
transfer(Account fromAccount, Account toAccount, long amount) {
    synchronized(fromAccount) {
        synchronized(toAccount) {
            // 转账逻辑...
        }
    }
}
```

为什么说这种是不那么容易发现的死锁呢？因为只有在两个人互相转账时，才会产生死锁。如用户A、用户B在同一时刻发起给对方转账的请求，这样两个线程中会分别锁住用户A和用户B，而又同时在等待对方账户被解锁。从而产生死锁问题。

#### 7.2.2.1 给资源编号

一种解决方法是，给资源编号。互相转账时，保证两人上锁顺序一致。比如可以按照账号哈希值的大小顺序开始上锁；如果担心哈希冲突，那么直接比较用户名，按字符串顺序上锁也可以。这样不光能保证两人互相转账，两人以上的互相转账，比如A->B->C->A这种转账也可以解决。

#### 7.2.2.2 使用轮询锁

> 《Java并发编程实战》13.1.1节

另一种解决方法是，可以按照相反的顺序获取锁，但仅仅尝试一次。若一次没有获取到，则当前线程随机休眠一段时间后，再次尝试。这既保证了不会无限阻塞造成死锁，同时，随机休眠机制也保证了不会因两线程同时谦让造成活锁。如下：

```java
// 入参：操作人、收款人、转款金额
transfer(Account fromAccount, Account toAccount, long amount) {
    while(true) { // 轮询锁，避免死锁
        fromAccount.getLock().tryLock(); // 每个账户内维护自己的锁对象
        try {
            toAccount.getLock().tryLock();
            try {
                // 转账逻辑...
            } finally {
                toAccount.getLock().unlock();
            }
        } finally {
            fromAccount.getLock().unlock();
        }
        Thread.sleep(randomNum); // 随机休眠避免活锁
    }
}
```

### 7.2.3 同步方法内调用另一个同步方法

这是另一种更不容易被发现的死锁。两个对象互相持有对方的引用，且方法同时调用了对方的同步方法。如下：

```java
class A {
    public synchronized void doSomething(B b) {
        b.doSomething(this);
    }
}

class B {
    public synchronized void doSomething(A a) {
        a.doSomething(this);
    }
}
```

（请忽略其中的递归调用问题）当A和B对象同时调用各自的doSomething()方法时，首先都会成功获取到各自对象实例的锁，然后尝试获取对方对象实例的锁，于是产生死锁。

因此，如果在持有锁的情况下调用某个外部方法，尤其是外部的synchronized方法，那么就需要警惕死锁。



### 7.2.4 资源死锁

资源死锁与前面的死锁类似，只不过抢夺的是“资源”，比如数据库连接池的连接、线程池中的线程。

通常会采用信号量机制来实现资源池为空时的阻塞行为。如果一个任务需要连接两个库，且无法保证每次请求资源的顺序相同，则可能产生死锁问题。



### 7.2.5 阻塞队列中的死锁

这个场景来自于双锁阻塞队列（LinkedBlockingQueue）源码中对死锁问题的处理。

一个由链表组成的队列，为了线程安全，不允许多线程同时入队、同时出队；但为了提高性能，允许入队和出队同时进行。因此设置了两把锁：入队（put）锁和出队（take）锁。

入队和出队的操作会首先抢到自己对应的锁，然后执行入队/出队的操作，最后释放这把锁。同时，若put操作检测到之前队列为空，则唤醒一个take操作；若take操作检测到之前队列已满，则唤醒一个put操作。

至此，死锁问题产生：由于唤醒操作（ `Object#notify` 或 `Condition#signal`）需要首先获取到这个锁才能执行，因此若处理不当，可能产生：put线程持有put锁，同时由于想要唤醒一个take操作，而抢夺take锁的情况。这就满足了死锁的**保持且等待**的条件。若这时再有另外一个take线程执行对称的操作，持有take锁想唤醒put操作，又满足了死锁的**循环等待**的条件。就产生了死锁问题。

如何解决死锁问题呢？可以通过“先释放一个锁，再去抢另外一个锁”的方式，来**破坏保持且等待**条件。take线程想要唤醒put操作，首先需要释放自己的put锁；同理，put线程也一样。这样就可以打破死锁了。



## 7.3 死锁的避免与诊断

**开放调用：对加锁逻辑进行封装**

在调用某个方法时不需要持有锁，这种方式称为开放调用。可以通过开放调用来避免死锁。

开放调用并不是真的没有锁——它只是把上锁流程封装进方法内部了，对调用方而言是看不到的。这样对调用方而言，分析死锁问题要比之前没有封装的程序容易得多。

Java大部分线程安全的类，使用的都是这种开放调用（使用ReentrantLock在内部上锁）。而像Vector这种古老的线程安全类，就没有使用这种方式，而是直接在方法上加synchronized。



**支持定时获取锁**

ReentrantLock中支持定时获取锁（`tryLock(timeout)` ），可以使用它来代替synchronized。若线程没有在指定时间内获取到锁，就会返回一个失败信息。

具体实现思路是：若获取锁超时，可以释放这个锁、同时释放该线程持有的全部锁，在一段时间后再次尝试。这样消除了死锁发生的条件，使程序恢复。



**通过线程转储信息分析死锁**

JVM自带功能，死锁检测。如jdk中的jconsole工具可以用于检测死锁。



## 7.4 线程饥饿和活锁

除了死锁以外，线程饥饿和活锁是其他两种可能导致程序无法正常执行的场景。

### 7.4.1 线程饥饿

由于线程无法访问它所需要的资源而不能继续执行时，就发生了饥饿。

对线程的优先级使用不当，会导致线程饥饿问题。CPU会优先执行那些优先级更高的任务，而对那些优先级低的任务，即使它们已经等待了好久，CPU仍然不会给它们分配时间。

因此，我们尽量避免使用线程优先级，因为这会增加平台依赖性，并可能导致线程饥饿问题。

### 7.4.2 活锁

线程不断重复执行相同的操作，而且总会失败，这就是活锁。当多个相互协作的线程都对彼此进行响应，从而修改各自的状态，并使得任何一个线程都无法继续执行时，就发生了活锁。这就像两个过于礼貌的人面对面相遇：他们都彼此让出对方的路，然后又在另一条路上相遇了。

要解决活锁问题，需要在重试机制中引入随机性。比如随机休眠一段时间后再次尝试，而不是两个线程休眠同样一段时间。



# 8. 性能与可伸缩性

> 《Java并发编程实战》第11章

## 8.1 对性能的思考

通常评价一个程序的性能，可以分为两大类：运行速度，和处理能力。

* 运行速度：表示程序的响应时间，即一批请求过来，每个请求需要耗费多久才能处理完成。通常在性能指标中使用tp99、tp999来表示99%或99.9%的请求的最慢处理时间；
* 处理能力：表示程序的吞吐量，即在计算资源一定的情况下，能处理多少请求而不宕机。通常在性能指标中使用qps、tps来表示每秒处理查询、处理事务的请求量。

我们为什么要使用多线程技术？毕竟，引入多线程后，程序复杂度提升，互斥锁的操作、线程的阻塞唤醒、创建和销毁，对于单线程来说都是额外的开销。如果对多线程使用不当，这些开销甚至会超过多线程所带来的性能提升，得不偿失。

举个例子，对于一个最简单的增删改查的web应用，在用户量较少的情况下，可以把MVC（展示、逻辑、持久化）三层都放在一个应用中。这样，三层之间的数据传输几乎没有任何延迟，也不需要将计算过程分解到不同的抽象层次，因此能减少许多开销。

然而这种单体应用很容易达到处理能力的极限。当用户量增多时，需要应用程序提供更高的qps，这对于单机的应用很难达到。因此，我们通常会接受在每个请求上执行更长时间（**降低响应性**）或消耗更多的计算资源，来换取**更高的处理能力**。比如，把单体服务拆分为微服务，这样会每个请求都增加了网络传输的消耗，但给整个程序带来了巨大的可扩展性。

**对于高并发的程序来说，能处理多少用户量（可伸缩性、吞吐量、生产量），要比单个请求的响应时间（运行速度）更重要。**

当进行程序优化时，经常会通过增加一种成本，来降低另一种开销。比如，增加网络传输成本来换取可伸缩性；增加缓存层来降低服务时间；降低代码可读性来换取更高效的算法等等。因此，在动手优化之前，一定要利用各种性能测试工具来保证，优化出来的性能，要大于为了优化而牺牲的成本。

## 8.2 Amdahl定律

参考资料：[阿姆达尔定律](https://www.cnblogs.com/zhangzefei/p/9775079.html) 

Amdahl定律主要用于计算系统可以得到的最大期望改进。经常用于并行计算领域，用来预测一个系统的理论最大加速比。在性能调优问题领域，利用此定律有助于解决或缓解性能瓶颈问题。

大多数并发程序都与种庄稼的过程有相似之处，他们都是由一系列的并行工作和串行工作组成的。如，程序要从共享的队列中获取任务，这是没法并行化的（想想阻塞队列中的take锁），就像庄稼的生长过程，增加再多的工人也不能提高庄稼的生长速度；而对于程序中的单个任务处理，可以被并行化，就像每一亩庄稼的收割一样，可以同时由多人完成。

假设一个程序，需要串行执行的部分占比为 $F$ ，那么可以被多线程并行的部分就是 $1-F$ 。假设我们用 $N$ 台服务器来并行执行 $1-F$ 这一部分程序，那么能达到最高的加速比为：
$$
Speedup \leq \frac{1}{F+\frac{1-F}{N}}
$$
加速比的意思是，在指定 $N$ 台服务器的情况下，相比于单线程执行，能提高的效率。假如加速比是3，就是说多线程环境下只用 $\frac13$ 的时间就能完成单线程相同的工作。

这个公式可以理解为：**加速比=单线程算法耗时 / 多线程算法耗时**。

其中单线程算法耗时是 $F+(1-F)=1$，而多线程算法耗时，将 $1-F$ 的部分放在 $N$ 个服务器上并行执行，因此平均时间是 $\frac{1-F}{N}$ 。

下图横轴是并发数（服务器数量），红线是在不断增大并发数时的加速比：

![image.png](https://i.loli.net/2019/09/27/9G7rbEXP8KxIZCA.png)



从公式和图中可以看出：

* 当 $N$ 趋于无穷大时，最大加速比趋近于 $\frac1F$ ；
* 随着线程数增大，加速比的增速变得越来越低；
* 当线程数继续增大，甚至会导致由于线程频繁切换，在后面出现加速比下降的情况（图中没显示）；

因此，要想提高服务的性能，仅仅堆机器是不够的，还要想办法降低串行执行的占比 $F$ 。

## 8.3 优化串行部分

### 8.3.1 哪些是串行部分？

应用程序中可能存在多个必须串行执行的部分。降低程序中必须串行部分的占比，是性能优化的重要一步。

* 线程池获取任务：线程池中使用到了阻塞队列，由于要保证线程安全，无法多线程同时从队列取任务。因此，线程池是并行执行任务，但获取任务时是一个必须串行的过程；
* 线程池执行结果处理：提交到线程池中的所有任务，都一定会产生某种结果，比如计算结果放入共享内存中、计算结果入库、记录日志等。这些结果通常也是串行的部分。

对于这些不得不串行执行的代码，它们都是通过锁来控制的。一个线程抢到锁才能执行，释放锁后才能把资源交给下一个线程。在并发程序中，对可伸缩性的最主要威胁就是独占方式的资源锁。如何优化这把锁呢？优化时可以有几种思路：

* 减少锁的持有时间；
* 降低锁的请求频率；
* 使用带有协调机制的独占锁，来获得更高的并发性。

下面几节讲具体的优化策略。

### 8.3.2 减少锁的竞争

**缩小锁的范围**

将与锁无关的代码移出同步代码块，尤其是一些耗时的、可能阻塞的操作；根据Amdahl定律，这种方法降低了串行代码的占比 $F$ 。

**减小锁的粒度**

即锁的拆分。如果一个锁需要保护多个相互独立的状态变量，那么可以将这个锁分解为多个锁，每个锁只保护一个变量。这样，将锁分解以后，每个新的细粒度锁上的访问量将比最初的访问量更少，从而降低每个锁被请求的频率；

书中将其分为两类，锁分解和锁分段。区别是：

* 锁分解是从业务上拆分，比如拆分用户锁和订单锁（当然这要详细分析业务，确保两处不需要保证原子性操作）；
* 锁分段是从技术上拆分，比如ConcurrentHashMap把整个数据集拆分为16把锁。而jdk1.8中的ConcurrentHashMap拆分的更细，解除了16把锁的限制，数组中的每一个元素都单独成锁。

### 8.3.3 避免热点域

当每个操作都请求多个变量时，锁的粒度很难降低。这是性能与可伸缩性之间相互制衡的一个方面，一些常见的优化措施，例如将一些反复计算的结果缓存起来，都会引入一些热点域，而这些热点域往往会限制可伸缩性。

个人理解，热点域产生的问题来源于：业务上需要这些热点域与各种操作保持原子性，或者叫数据的强一致性。试想，好不容易把一把锁拆成了多把锁，可以多线程并行执行了，结果出来一个热点域，每个线程执行前还要抢这个热点域的锁。这就让锁拆分的优化变得无效。

典型的例子是ConcurrentHashMap中的元素数量size。若多线程同时操作时都要抢size变量的互斥锁，就使得分段锁的设计变得无效。为了避免这个问题，ConcurrentHashMap中的size会遍历每个分段，并对其中的元素数量相加，而不是维护一个全局计数。

### 8.3.4 放弃独占锁

另一种降低竞争锁的影响的技术就是放弃使用独占锁，而是使用一种友好并发的方式来管理共享状态。

* 读写锁：jdk中的实现是 `ReadWriteLock` ；TODO 读源码
* 原子变量：jdk中实现了利用CAS操作来保证原子性操作的数值包装类，比如 `AtomicInteger` 等。它们提供了更细粒度的锁，因此伸缩性更高；
* 线程安全的类：jdk对于很多数据结构，都提供了线程安全的版本，且它们并不是简单粗暴的上锁，而以更细粒度的锁来保证线程安全性。如：ConcurrentLinkedQueue非阻塞队列。TODO 读源码

### 8.3.5 向对象池说“不”

随着JVM不断迭代， 对象分配和GC的操作变得越来越快，因此没有必要为了节省对象分配（即new一个对象）的时间，而将对象缓存在池子中，反而这种操作会降低性能。

在并发程序中，当在线程内分配新的对象时，JVM会利用逃逸分析检测出该对象不会逃逸出该线程，因此通常会使用线程本地内存（CPU缓存或栈内存上分配），不需要在堆内存上进行同步。相反，如果使用了对象池，那就需要通过某种同步机制来协调多线程对池子的访问，从而可能使某个线程被阻塞。而这种阻塞的开销是内存分配操作开销的数百倍。

所以，不要使用对象池。



# 9. 并发程序的测试

> 《Java编程实战》第12章

对一个会被多线程访问的类而言，有两种评价指标：第一种是**正确性**。包括功能正确、不会出现线程安全问题、不会因持有过多对象引发OOM异常等；第二种是**性能**，即上一章讲的响应时间、吞吐量。

下面依次来讲解如何对这几个指标进行测试。这几节会以 `LinkedBlockingQueue` 类为例，编写测试代码。

## 9.1 基本的单元测试

目的是测试单线程环境下，该类是否能正确工作。这里的正确工作，就是验证是否符合5.2节讲到的各种条件。

以阻塞队列为例，需要测试：新创建的队列一定为空；插入与队列最大容量相同个数的元素后，队列一定已满。

## 9.2 阻塞测试

目的是测试单线程环境下，该类能否在满足阻塞/唤醒条件下，正常的阻塞/唤醒。

以阻塞队列为例，需要测试：当队列为空时，take操作需要被阻塞；当队列已满时，put操作需要被阻塞。

现在我们知道了想要达到的阻塞效果，那么测试程序应该如何编写？可以采用这样的思路：不在主线程中阻塞，而是创建一个子线程去阻塞，并捕获其中断异常（InterruptedException）。主线程可同时测试子线程的阻塞、响应中断。如下，详细代码见：com.learning.project.fzk.blockingQueue.test.TestBlock。

```java
MyLinkedBlockingQueue<Integer> queue = new MyLinkedBlockingQueue<>(10);

Thread thread = new Thread(() -> {
    try {
        queue.take();
        Assert.fail("阻塞被唤醒");
    } catch (InterruptedException e) {
        System.out.println("线程被成功中断");
    }
});

thread.start();
Thread.sleep(1000); // 测试阻塞
thread.interrupt(); // 测试响应中断
thread.join(1000);
Assert.assertFalse(thread.isAlive());
```

其中 `join()` 方法的作用是，让主线程阻塞，等待子线程执行结束后，主线程继续执行。在这里的作用仅仅是正确地结束测试。

注：书中没有介绍唤醒的测试，不过方法应该与阻塞测试类似。

## 9.3 线程安全测试

书中称为安全性测试，目的是测试在多线程环境下，该类是否会由于线程之间的交叉执行，导致各种异常情况。

以阻塞队列位列，测试线程安全的思路是：多线程同时入队、出队，最终验证结果。可以使用jdk自带的线程池来模拟并发，如：对于一个容量为10的阻塞队列，使用10个线程，同时执行1000000次入队、出队操作。每次入队的元素是一个随机数，且每次入队出队，都将插入/移出的随机数累加。若最终多线程插入/移出的累加数字相等，则说明没有产生线程安全问题。

代码见：com.learning.project.fzk.blockingQueue.test.TestThreadSave。

**如何迫使多线程同时执行？**

看代码会发现，如果多线程分别启动，那么一定会有线程先开始执行，后启动的线程才会继续执行。这样就产生了一小段串行的代码。可以使用线程栅栏的结构CyclicBarrier，来避免这种情况。

5.2.2.6节中讲过此结构，它可以先执行完的线程等待一会，等所有线程都处于同一起跑线后，再让它们同时继续执行。在这里的作用是，迫使所有线程处于同一起跑线，然后同时开始take/put，将程序的并发程度最大化。

**如何产生更多的交替操作？**

由于并发代码中的大多数错误都是低概率事件，且很难重现，只有对于特定的交叉执行顺序才会引发某种线程安全问题（如数据不一致、死锁等）。因此我们需要一种方法提高交替操作的数量，而不是测试时让CPU尽可能快速的执行完所有代码。这样可以更有效地搜索程序的状态空间。

有以下两种方式：

* 使用 `Thread.yield()` 方法，该方法是“线程让步”方法。它告诉CPU当前线程已经执行完主要的任务了，可以尝试让出自己对CPU的使用权了。因此该方法会产生更多的上下文切换。示例：

  ```java
  // 对于一个转账逻辑，必须保证原子性。因此可以在多个操作之间插入yield()方法，来证明真的保证了原子性
  synchronized void transfer(Account fromAccount, Account toAccount, long amount) {
      fromAccount.setBalance(fromAccount.getBalance() - amount);
      
      if(random.nextInt(1000) > 500) { // 随机yield
          Thread.yield();
      }
      
      toAccount.setBalance(toAccount.getBalance() + amount);
  }
  ```

* 使用 `Thread.sleep()` 方法。执行起来会慢一些，但相比于yield更可靠。因为yield不能保证当前线程一定让出CPU的使用权，只是一种尝试。

## 9.4 响应性测试

响应性是指，每个请求需要花费多长时间来返回结果。根据响应性数据，我们可以检测服务质量，比如可以计算：100ms内成功返回结果的百分比是多少。我们可以通过修改上面的程序，添加一个计时功能来记录请求消耗的时间。

锁是否公平，是影响响应性的重要因素，如下：

![a](https://i.loli.net/2019/10/01/zdVvAJYnPHi6klS.png)

|              | 公平锁 | 非公平锁 |
| ------------ | ------ | -------- |
| 平均响应时间 | 更长   | 更短     |
| 响应时间方差 | 更小   | 更大     |

因为对于公平锁，几乎每个请求都要进入阻塞状态，等抢到锁后再被唤醒，每个请求的执行流程都是这样，因此方差更小；而阻塞/唤醒操作是一个耗时操作，因此平均响应时间更长。

而非公平锁不必每个线程都进入阻塞状态：当一个线程请求非公平锁时，如果在发出请求的同时，该锁的状态变为可用，那么这个线程将跳过队列中所有的等待线程，抢到这把锁。因此相比于公平锁，节省了阻塞唤醒的开销。



# 10. 构建自定义的同步工具

java.util.concurrent包中提供了很多并发相关的工具类，比如下面列出的编写并发程序中常用的工具类：ReentrantLock、ReentrantReadWriteLock、Semaphore、CountDownLatch、SynchronousQueue、FutureTask，它们底层都是基于内置条件队列、显式Condition对象、AbstractQueuedSynchronizer来实现的。

本章将首先介绍这些底层组件，然后看下常见的并发工具类的实现原理。

TODO 当然还有其他不基于这些的工具类，将在以后整理。

## 10.1 条件队列

《Java并发编程实战》14.1节中，从零开始构建了一个线程安全的条件队列。一开始使用了互斥锁synchronized，但无法做到阻塞；又使用了休眠的方式来模拟阻塞，但休眠时间不好掌握；最终使用了wait/notify实现了一个线程安全的条件队列，也就是简化版的阻塞队列。

**过早唤醒问题**

在实现条件队列的入队出队互相唤醒逻辑时，有一点需要特别注意：过早唤醒问题。拿入队操作来举例，伪代码如下：

```java
put(E e) {
    lock(); // 上锁
    while(isFull()) { // 已满则阻塞
        this.wait();
    }
    enqueue(e); // 元素入队
    unlock(); // 解锁
    notifyTake(); // 唤醒take线程
}
```

注意其中的“已满则阻塞”处的代码，使用了一个循环。这是因为，在该线程被唤醒时，需要重新抢夺到锁，才能继续执行；而很可能就在这个时间空隙内，其他线程抢锁修改数据解锁一气呵成， `isFull()` 的条件再次变为假。因此，每次被唤醒后必须要再次判断条件。

可以参照阻塞队列的源码，其中无论take还是put，只要是可能阻塞的方法，阻塞逻辑都添加了一层循环。

## 10.2 AQS简介

AbstractQueuedSynchronizer(AQS)是一个用于构建锁和同步器的框架，很多并发工具类的实现基础。ReentrantLock、ReentrantReadWriteLock、Semaphore、CountDownLatch、SynchronousQueue、FutureTask等都是基于AQS实现的。

个人理解，AQS实际上是实现了最基本的PV操作。AQS类内部会维护一个整数状态值state（即信号量），并提供对状态值的获取（aquire）和释放（release）操作（即生产-消费操作）。而翻译为信号量的Semaphore工具类，其实是对AQS的封装，它实现了公平/非公平的信号量唤醒机制。

状态值（信号量）是一种资源，它的业务含义随着使用在不同地方而不同。

* 对于ReentrantLock来说，它表示所有者线程以重复获取该锁的次数（回想可重入锁的概念）。初始为0，同一个线程上一次锁，信号量+1；
* 对于CountDownLatch来说，它表示尚未运行完成的线程个数。初始值为要并行执行的线程个数n，每当一个线程执行完，通知它一下，它就会将信号量减1，并阻塞该线程。直到信号量为0时，唤醒全部线程；
* 对于ReentrantReadWriteLock来说，它表示正在使用文件的进程类别个数。读是一类，写是一类，因此仅可能是0或1。也可以理解为，文件是否正在被使用。信号量初始为1，因为没有任何人占用文件；一旦有线程开始读或写，则信号量变为0，直到读/写操作结束；
* 对于Semaphore来说，它本身就是一个加强的信号量机制，因此也表示剩余的许可数量；
* 对于FutureTask来说，它表示任务的状态（未开始、运行中、已完成、已取消）。

有了AQS这样的信号量机制，我们就可以解决各种并发问题了。注意，juc包中所有的并发工具类都没有直接扩展（继承）AQS，而是将它们相应的功能委托给私有AQS子类来实现。这是为了防止调用方误调用AQS的内部方法，打破内部数据一致性。

## 10.3 AQS源码阅读

TODO

## 10.4 并发工具类的源码阅读

TODO









# 附录

## 一、分享大纲

### （1）2019.9.15期

- 线程安全的五个级别
  - Java中对应的哪些级别，分别举例
- 如何实现线程安全？
  - 【插播】操作系统相关知识
    - 用户态与内核态；
    - 线程切换、阻塞与唤醒的性能问题；
  - 互斥（非阻塞）同步：悲观锁
    - synchronized
    - ReentrantLock
    - 二者对比，优缺点
  - 非阻塞同步：乐观锁
    - CAS操作，及其ABA问题
  - 无同步方案
    - 完全在栈上分配内存；
    - ThreadLocal；
- 锁优化
  - 自旋锁；
  - 适应性自旋；
  - 锁消除；
  - 锁粗化；
  - 轻量级锁；
  - 偏向锁；

- CPU的并发问题
  - 【插播】CPU的l1、l2缓存，进程本地内存；
  - 缓存一致性问题，与缓存一致性协议；
  - 重排序优化问题，与代码依赖关系分析；
- Java内存模型
  - JMM逻辑模型与堆栈内存的关系；
- JVM并发问题的解决
  - 8种原子性操作
  - 8个先行发生原则
  - volatile如何保证可见性、有序性？
  - synchronized如何保证原子性、可见性、有序性？



### （2）2019.10.13期

* 如何设计一个线程安全的类
  * 变量约束条件分析
  * jdk中提供了哪些线程安全的类？
    * map结构、list结构、队列、闭锁CountDownLatch、信号量Semaphore、栅栏CyclicBarrier；
  * 设计一个本地缓存工具类
* 死锁
  * 死锁场景：典型死锁、互相转账死锁、方法调用死锁、资源死锁、阻塞队列中的死锁
  * 避免死锁：定时获取锁；
  * 线程饥饿
  * 活锁



### （3）2019.10.20期

* 遗留问题
  * 5.3.3 future与task变量名问题
    * 双重检查的说明
  * 5.3.4 改进本地缓存结构：使用 `computeIfAbsent` 
* 7.2.2.2 轮询锁，lock改为tryLock
  * 7.2.3 
  
* 性能、可伸缩性
  * Amdahl定律
  * 根据Amdahl定律进行优化
    * 找出串行部分的代码
    * 减少锁竞争
    * 避免热点域
    * 放弃独占锁
* 并发程序的测试
  * 单元测试
  * 阻塞测试（唤醒测试）
  * 线程安全测试
  * 响应性测试
* 构建自定义的同步工具
  * 阻塞队列的过早唤醒问题
  * AQS简介



## 二、一些还没有想好应该放在哪里的笔记

**synchronized不要修饰web服务中的方法**

当synchronized修饰方法时，锁住的是这个实例对象。而对于基于spring的web服务而言，默认都是单例的bean，一旦锁住了这个对象，就说明该方法无法并发服务于多个用户，只能服务完一个用户，这个bean被解锁后，下一个用户的请求才能获取到这个bean，这对web服务的体验带来毁灭性的打击。

其次，对于多核服务器，锁住了一个对象，该对象就无法被其他CPU使用了，造成一人干活，多人围观的情况。同样会降低程序的并发性。



**为什么阻塞和唤醒操作一定要在拿到锁（synchronized/ReentrantLock）后才能执行？**

答：多线程环境下，无法保证notify一定先行发生于另一个线程的wait。试想这样一种情况：线程A第0秒执行了wait，需要耗费2秒；线程B在第1秒执行了同一个锁对象的notify，但发现并没有线程处于wait状态。于是，等线程A的wait执行完后，它将永远阻塞。

这种设计是为了通过锁的方式，保证notify一定先行发生于其他线程的wait，保证notify和wait都是原子操作。



## 三、相关资料

《深入理解并行编程》：[原文](https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook-1c.2018.12.08a.pdf) [中文版](http://ifeve.com/wp-content/uploads/2013/05/深入理解并行编程V1.0.pdf) [来源](http://ifeve.com/perfbook/) 

内存屏障相关论文： [Memory Barriers: a Hardware View for Software Hackers](http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf) 

