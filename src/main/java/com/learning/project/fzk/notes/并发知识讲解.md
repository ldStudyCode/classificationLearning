[TOC]

> 参考书籍：
>
> 1. 《深入理解Java虚拟机》12、13章
> 2. 《Java并发编程实战》



# 1. 线程安全

## 1.1 定义

线程安全的定义：

当多线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。

换个说法，多线程以任意的顺序来执行、线程之间互相穿插执行，最终的结果就跟串行执行是一样的，就可以称为线程安全。

## 1.2 线程安全级别

线程安全级别分为如下5类，安全性由强至弱排序如下：

### 1.2.1 不可变

不可变的对象一定线程安全。不可变的对象一旦被构造出来，永远也不会产生多线程中不一致的现象。

Java实现不可变的方法：

* 8种基本类型：加final；
* 对象：把其中所有属性加final。
  * 例：String，和Integer、Double等数字类都是不可变对象。

### 1.2.2 绝对线程安全

不管运行时环境如何，调用者都不需要任何额外的同步措施。

绝对线程安全并不是我们通常意义上的线程安全，因为它不光要保证单次调用下的安全，还要保证连续调用下的安全。而在调用方没有其他保护的情况下，这基本上是不可能实现的。

例如，对于一个 `AtomicInteger` 对象 `i` ，若多个线程同时执行 `i = i.get() + 1` ，是无法达到线程安全的，因为这个操作会先取值、再加1、最后赋值，这三步操作会在多线程下交叉执行，一定会产生线程安全问题。

改进的方法是，将 `i = i.get() + 1` 语句替换为 `i.getAndAdd(1)` ，将三步操作放到一个原子性操作中去。

### 1.2.3 相对线程安全

相对线程安全，就是我们通常意义上所讲的线程安全。它需要保证对这个对象**单独操作**是线程安全的，但对于一些特定顺序的连续调用，需要在调用端使用额外的同步手段来保证正确性。

Java中大多数声明为线程安全的类，指的都是这一类。

### 1.2.4 线程兼容

指对象本身不是线程安全的，但可以通过调用方正确使用同步手段，来保证对象在并发环境下安全使用。

Java中大部分没有声明为线程安全的类，指的都是这一类。

### 1.2.5 线程对立

无论调用方是否采取同步措施，都无法在多线程环境中安全地并发使用。

由于Java语言天生具备多线程的特性，这一类在Java中基本不会出现。



## 1.3 线程安全的实现方法

### 1.3.1 互斥同步

互斥同步保证同一时刻只被一个线程使用。

Java中，synchronized关键字和ReentrantLock都可以实现互斥。

#### 1.3.1.1 synchronized

* **锁住了谁？**

  若代码中明确了锁对象，锁住的就是这个对象；

  修饰了一个方法：若是静态方法，则锁住了Class对象；若是实例方法，则锁住了这个对象实例；

* **上锁的流程？**

  synchronized关键字编译后，会在同步块的前后分别形成moniterenter和moniterexit这两个字节码指令，这两个指令内部执行的，就是后面要讲的CPU原子操作：lock和unlock。

  执行moniterenter时，首先尝试获取对象的锁：若对象没有被锁定，或当前线程已经拥有了这个对象的锁（此处表示可重入），则锁的计数器+1。相应的，在执行moniterexit时，会将计数器-1。计数器为0时，锁被释放。

  若获取对象锁失败，则线程阻塞，直到对象锁被另外一个线程释放，且自己抢到为止（非公平）。

* **重量级锁**

  Java线程是1:1映射到操作系统原生线程之上的，因此如果要阻塞或唤醒一个线程，都需要操作系统帮忙完成。这就需要操作系统从用户态转换到内核态中，而这个操作是开销比较大的。所以说synchronized是一个重量级锁。

  为了优化synchronized的性能，JVM会在阻塞线程之前加入一段自旋等待的过程，避免频繁切入到内核态中。将在下面章节讲到。

#### 1.3.1.2 ReentrantLock

与synchronized作用相似，也是可重入锁，但提供更多可配置选项。相比于synchronized的一些高级功能如下：

* **等待可中断**：定时获取锁，获取不到则自动中断（`tryLock(timeout)` 方法）、定时阻塞（ `Condition#await(timeout)` 方法）；
* **可实现公平锁**：默认非公平锁，但可通过构造函数设置为公平锁。即等待时间越长的线程，越先从阻塞中被唤醒；
* **锁可以绑定多个条件**：利用锁创建多个Condition对象，给阻塞中的线程分组。实例：单锁的阻塞队列 `LinkedBlockingDequeue` ，同一把锁，分为了put和take两个阻塞条件，可指定唤醒某一组中的一个线程。

#### 1.3.1.3 互斥同步的特点

互斥同步又称“阻塞同步”，因为抢不到锁的线程会阻塞。而线程的阻塞和唤醒是需要OS进入内核态的，会有性能问题。

从处理方式上看，互斥同步是一种悲观锁。它认为，只要不上锁，一定会出现问题，无论共享数据是否真的会出现竞争。跟它对应的是非阻塞同步，是一种乐观锁策略。



### 1.3.2 非阻塞同步

非阻塞同步是一种基于冲突检测的乐观锁。具体操作是：先不上锁，直接操作，如果没有其他线程竞争共享数据，那么操作就成功了；如果共享数据有竞争，就采取补偿措施（不断重试，直到成功为止）。

按照这种逻辑，非阻塞同步不会让线程阻塞、唤醒，相比于阻塞同步提高了性能。

为了实现非阻塞同步，必须保证“冲突检测+操作”的步骤具有原子性。但如果这里再使用互斥同步就失去非阻塞的意义了，因此只能靠CPU指令来保证这件事。CPU提供了CAS操作（compare and swap，比较并交换），比较就是冲突检测，交换就是操作。

一个利用CAS实现计数器+1的方法实现如下（AtomicInteger中的实现类似）：

```java
increment() {
  while(true) {
    int current = get(); // 当前数值，即待比较的数值
    int next = current + 1; // 计算后的结果
    if(cas(current, next)) { // 调用CPU保证的原子性cas操作
      return;
    }
  }
}
```

CAS操作的一点小问题：ABA问题。初次读取时是A，比较时仍然也是A，但可能在读取后、比较前，变量值从A改成了B，又迅速改回了A。CAS操作就会误认为它是没有被改变过。但大部分情况下，ABA问题不会影响程序并发的正确性，因此还是可以放心使用CAS的。



### 1.3.3 无同步方案

如果一段代码天生就是线程安全的，也就不需要通过特殊手段来保证同步了。有如下三类：

* 不可变对象：final的8大基本类型数据，以及成员变量都是final的对象，它们一定是线程安全的；

* 可重入代码：不依赖堆内存上的共享资源，所有的状态都是存在于线程独立的栈内存中的；
* 线程本地存储：如果能保证数据只在同一个线程内被使用，而不会泄露给其他线程，这样也是可以保证安全的。常见的场景有：
  * Web交互模型中的“一个请求对应一个服务器线程”，这样我们在写服务器端的Controller方法时，无需考虑线程安全问题；
  * ThreadLocal：实现线程本地存储，每个线程只操作自己的变量，而不会泄露给其他线程。



# 2. 锁优化

参考资料：[浅谈偏向锁、轻量级锁、重量级锁](https://www.jianshu.com/p/36eedeb3f912) 

## 2.1 自旋锁、适应性自旋

自旋锁和适应性自旋，都是对互斥同步（悲观锁）（synchronized、ReentrantLock都适用）的一种优化方式。

**自旋锁**

由于互斥同步在无法获取到锁时，线程会进入阻塞状态，而阻塞需要操作系统进入内核态来完成，开销较大。而许多应用上，共享数据的锁定状态只会持续很短一段时间，为了这段时间去挂起和恢复线程不值得。

自旋锁做的事情是：在阻塞线程之前，尝试一定次数获取锁的操作，如果一定次数（默认10次）后，仍然无法获得锁，再由OS帮忙进入阻塞状态。

自旋锁的意义是，减少了线程的阻塞、唤醒的次数，将那些“即将获取到锁的线程”阻塞的性能消耗，转移给了CPU，提高了性能。

**适应性自旋**

jdk1.6以后，引入了适应性自旋。是指更聪明地决定自旋次数，而不是简单粗暴的默认10次。

如果在同一个锁对象上，自旋等待刚刚成功获得过锁，且持有锁的线程正在运行中，那么JVM会认为这次自旋很有可能再次成功，因此允许它自旋次数更多一些，如100次。

若一个锁对象很少通过自旋成功被获得过，那么就会减少或省略掉自旋过程，以避免浪费CPU资源。

## 2.2 锁消除

锁消除是指，对于一些代码上要求同步，但被检测到不可能存在共享数据竞争的锁进行消除。如果检测到一段代码，堆内存的所有数据都不会被其他线程访问到，那就可以把它们当作栈上的数据对待，认为它们是线程私有的， 同步加锁自然就无须进行。

这项优化消除的锁，一般都不是程序员自己加上的，而是JVM对代码编译之后不小心加上的锁。也就是，JVM先优化了一波代码，不小心加上了锁，再使用锁消除，来把不必要的锁去掉。

举个例子：

```java
// 编译前，看起来没有同步的代码
public String concat(String s1, String s2, String s3) {
    return s1 + s2 + s3;
}

// 编译后，为了避免频繁生成新的String对象，优化为StringBuffer
public String concat(String s1, String s2, String s3) {
    StringBuffer sb = new StringBuffer();
    sb.append(s1);
    sb.append(s2);
    sb.append(s3);
    return sb.toString();
}
```

经过锁消除后，虽然在StringBuffer中仍然有加锁的逻辑，但实际上并不会真正加锁。

## 2.3 锁粗化

对于一系列连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥操作也会导致不必要的性能损耗。

具体的粗化方式是，把循环体内的加锁解锁逻辑，移动到循环体外部，这样执行整个循环，只需要加锁解锁一次就可以了。

## 2.4 轻量级锁

轻量级锁的目标是，减少无实际竞争的情况下，使用重量级锁产生的性能消耗，包括系统调用引起的内核态与用户态切换、线程阻塞造成的线程切换等。

轻量级锁使用CAS操作来代替互斥量的操作。若CAS操作成功，则轻量级锁获取成功；否则说明已经有线程获得了轻量级锁，说明发生了锁竞争，不适合继续使用轻量级锁，接下来膨胀为重量级锁，即互斥量。

缺点：若锁竞争激烈，那么轻量级锁会很快膨胀为重量级锁，那么维持轻量级锁的过程就成了浪费。

## 2.5 偏向锁

在轻量级锁的基础上继续优化：如果不仅没有实际竞争，而且自始至终，使用锁的线程只有一个，那么维护轻量级锁都是浪费的。因为每次获取轻量级锁，都要至少执行一次CAS操作。但偏向锁的目标是，在无竞争、只有一个线程使用锁的情况下，减少使用轻量级锁产生的性能消耗。

偏向锁只有初始化时需要一次CAS操作，会在对象头信息中记录这个线程标志owner。而当同一个线程再次获取锁时，不需要再次执行CAS操作，可以直接获取到锁。若其他线程想要获取这个锁，发现自己并不是owner，那么偏向锁就会膨胀为轻量级锁。

偏向锁的名字可以理解为，当第一个线程进来时，偏向锁就偏心地认为，这个线程就是唯一要获取这个锁的线程了。

## 2.6 逃逸分析简介

2.2节介绍的锁消除优化，会检测不可能被多线程访问的对象，然后把锁消除。这里使用的检测方法就是逃逸分析。

逃逸分析的基本行为就是分析对象的作用域。在一个方法中创建了对象，若这个指针作为参数传递到其他方法中，称为**方法逃逸**；若外部线程也能拿到这个指针，则称为**线程逃逸**。

如果能证明一个对象不会逃逸到方法或线程之外，就可以为这个变量进行一些优化：

* **栈上分配**：若一个对象不会产生方法逃逸，则可以将这个对象直接创建在栈内存的方法栈帧上。这样，方法结束，栈帧出栈，对象直接销毁。避免了创建在堆内存上触发GC的消耗；
* **锁消除**：若一个对象不会产生线程逃逸，则可以将这个对象的同步措施（加锁解锁）消除掉。好处是，减少锁操作的次数，以及获取不到锁时，操作系统进入内核态将线程阻塞所产生的性能消耗。当然对于这种情况，会有自旋锁和适应性自旋来优化；
* **标量替换**：标量是指八种基本类型以及reference类型的变量；聚合量是指由标量组合而成的变量，Java中的对象就是一种聚合量。若一个对象不会发生逃逸，则可以把这个对象（聚合量）拆散，不直接在堆内存上创建完整的对象，而是让对象的成员变量分配在栈上（栈上存储的数据，有很大概率会被JVM分配至物理机器的高速寄存器中存储）。

## 2.7 总结

锁的分类，从开销最小到开销最大，分别是：

1. 偏向锁（假定从始至终只有一个线程获取锁）；
2. 轻量级锁（假定竞争不激烈，使用CAS操作就可以）；
3. 重量级锁（互斥锁，需要OS切换内核态）；

当一个线程首次尝试利用synchronized获取锁时，会首先从偏向锁开始，当不满足偏向锁的假定时，会膨胀为轻量级锁；当仍然不满足轻量级锁的假定时，会膨胀为重量级锁。



对锁的优化方式：

* 自旋；
* 适应性自旋；
* 消除；
* 粗化。



# 3. CPU的并发问题及解决

关于并发，较好的学习方式是把硬件（CPU）级别的并发、和JVM的多线程并发结合来看。物理机和虚拟机在处理并发问题中，有很多相似的解决方案。

总体来看，有以下几点是相似的：

- 缓存一致性问题
- 重排序优化问题



## 3.1 缓存一致性问题

### 3.1.1 问题来源

CPU在运行时一定会与内存交互，如读取运算数据、存储运算结果等，这个I/O操作是很难消除的。由于内存读取速度与CPU运算速度之间有几个数量级的差距，为了不让I/O操作成为瓶颈，计算机在CPU和低速内存之间加了多级缓存（寄存器、l1、l2、l3 cache）：将运算需要使用到的数据复制到缓存中，让运算能快速运行，当运算结束后再从缓存同步回内存中，这样CPU就无须等待内存读写了。

CPU速度与常见操作的速度对比：假设CPU执行一条指令需要1秒，那么：

* 一级缓存读取：1.3秒；
* 互斥锁的加锁和解锁：65秒；
* 内存寻址：260秒；
* CPU上下文切换（即线程、进程的切换）：65分钟；
* SSD硬盘寻址：4.5天；
* 调用网络服务（http、RPC等，0.5ms）：15天；
* 磁盘寻址（10ms）：10个月。

参考资料：[让 CPU 告诉你硬盘和网络到底有多慢](https://cizixs.com/2017/01/03/how-slow-is-disk-and-network/) 

下图是硬件级别的内存模型：



![微信截图_20190829144728.png](https://i.loli.net/2019/08/29/s4fbVYj3pHnJlK2.png)



这样就会产生缓存一致性的问题：多个CPU同时把共享内存中的值复制到自己的缓存中，可能导致各自的缓存数据不一致，那么同步回主内存时，就无法确定以哪个CPU缓存中的值为准。



### 3.1.2 MESI协议

为了解决这个问题，操作系统在高速缓存和主内存之间加了一道“缓存一致性协议”，来保证多CPU同时操作一块共享内存时，不会产生数据不一致问题。

拿MESI协议来说明问题，它是这样解决多CPU之间缓存不可见问题的：

1. Core0修改变量v后，发送一个信号，将Core1缓存的变量v标记为失效，并将修改值写回内存；
2. Core0可能会多次修改变量v，每次修改都只发送一个信号（发信号时会锁住缓存间的总线），Core1缓存的变量v保持着失效标记；
3. Core1使用变量v前，发现缓存中的v已经失效了，得知v已经被修改了，于是重新从其他缓存或内存中加载。

以上即是MESI（Modified Exclusive Shared Or Invalid，缓存的四种状态）协议的基本原理，不算严谨，但对于理解缓存可见性（更常见的称呼是“缓存一致性”）已经足够。



## 3.2 指令重排序问题

### 3.2.1 问题来源

为了使CPU内部的运算单元尽量被充分利用，CPU可能会对输入的指令进行**乱序执行**优化，并在计算之后，将乱序执行的结果进行重组，保证该结果与顺序执行的结果是一致的。

问题来了：由于只能保证最终结果的一致性，并不保证代码逻辑的先后顺序与实际执行的先后顺序一致，因此，如果存在一个计算任务依赖于另外一个计算任务的中间结果，那么顺序性不能靠代码逻辑的先后顺序保证。

### 3.2.2 指令重排的原则

CPU是如何解决这件事的呢？CPU在进行指令重排时，必须保证：指令上下文的因果关系、依赖关系不发生改变。换句话说，只要不影响程序单线程、顺序执行的结果，就可以对两个指令重排序。举例：

```java
a++; b=a*2; c--;
```

可能会被重排为：

```java
a++; c--; b=a*2;
```

但一定不会出现：

```java
b=a*2; a++; c--;
```

因为对b的赋值操作依赖于a的计算结果，这样优化后改变了a和b的依赖关系。

Java虚拟机的即时编译器中，也有类似的指令重排序优化。



# 4. Java内存模型

对于Java，也有自己的内存模型（JMM，Java Memory Model）。这套关于并发的内存模型是一个逻辑模型，而之前讲的堆、栈、方法区等等是JVM在物理内存上的真实划分。稍后会说明这两套模型的对应关系。

JVM内存模型规定了，所有变量存储在**主内存**（类比上图的主内存），每条线程还有自己的**工作内存**（类比CPU高速缓存）。

线程工作内存中，保存了该线程操作的变量在主内存中的拷贝，线程对变量的所有操作（读取、赋值）必须先在工作内存中完成，然后再同步到主内存中去。线程不能直接读写主内存中的变量。

> 关于从主内存到工作内存的拷贝，对于一个大对象，只会拷贝当前线程需要操作的某一个属性，不会拷贝整个对象。原则是尽可能少的向工作内存中拷贝。

下图是JVM的内存模型，可以与上图对比：

![微信截图_20190829145732.png](https://i.loli.net/2019/08/29/HrxcPYpkVSTvqbO.png)



上图与JVM内存的物理划分的对应关系是：

* 主内存：对应堆内存，准确说是其中存储数据的部分；

* 工作内存：对应栈内存的部分区域，但为了获得更好的运行速度，JVM会让工作内存优先存储于寄存器和CPU高速缓存中，因为程序运行时主要访问读写的是工作内存。

因此上图是一个逻辑模型，我们其实不需要关心主内存和工作内存到底存放在什么位置，只需要分析这种模型产生的线程安全问题，以及如何解决它们。



# 5. JVM的并发问题及解决

按照这样的内存模型，就引发了线程安全问题（请与CPU缓存不一致问题类比）：多个线程同时操作一个对象，都拷贝到自己的工作内存中，无法保证工作内存中的数据一致性。若两个线程基于同样的拷贝对象进行操作，则先执行完的线程结果，一定会被后执行完的线程结果所覆盖。这就是常说的线程安全问题。

此外，多个线程都进行代码重排序，也会产生线程安全问题。



## 5.1 8种原子性操作

Java如何解决线程安全问题？JMM提供了8种原子性操作，我们熟悉的锁相关的关键字（volatile、synchronized），底层都是利用这8种原子性操作来完成的。如下图所示：

![36c022bed418ad653a2f22f15d2b468.jpg](https://i.loli.net/2019/08/29/tyK6Yb4DguSWEkw.jpg)

* lock（锁定）：作用于主内存对象，将对象监视器从0变为1，表示对象处于内存独占状态；
* unlock（解锁）：作用于主内存对象，将对象监视器从1变为0，表示对象无锁；
* read（读取）：作用于主内存对象，把主内存对象传输到工作内存中，以便load使用；
* load（载入）：作用于工作内存对象，工作内存中的变量指针，指向刚刚read到的内存区域；
* use（使用）：作用于工作内存对象，线程使用对象时，将对象的值传递给执行引擎；
* assign（赋值）：作用于工作内存对象，线程使用完对象，将计算后的新值赋给工作内存中的副本对象；
* store（存储）：作用于工作内存对象，把工作内存对象传输到主内存中，以便write使用；
* write（写入）：作用于主内存对象，主内存中的变量指针，指向刚刚store的内存区域。



## 5.2 先行发生原则

面对无数种多线程的实际场景，我们如何证明一段代码是线程安全的？

Java内存模型提供了八种天然的先行发生关系。这些先行发生关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，JVM可以对它们随意进行重排序。

也就是说，我们要证明一段代码是否线程安全，就要看每一对需要保证先后顺序的操作是否真的具有先行发生的关系。如果存在那么一对操作，调用顺序是A -> B，它们需要保证A的结束一定发生在B的开始之前，但无法从以下规则中推导出来，则说明这段代码存在线程安全的风险。

先行发生（happens before）原则如下：

* 程序次序规则：**写在前面的（或在循环、分支等逻辑靠前的）代码**一定先行发生；

  * 这就保证了在所有单线程环境下，不会由代码重排序引发安全问题；

* 管程锁定规则：**同一个锁对象的unlock操作**，一定先行发生于**下一次lock操作**（这里lock和unlock在代码上使用synchronized实现）；

  * 管程是对PV操作的封装，屏蔽了PV操作编写复杂和易错的缺点，在Java中就是synchronzied；

* volatile变量规则：**对一个volatile变量的写操作**，一定先行发生于**下一次读操作**；

  * 注意，这条规则保证了volatile具有可见性。
  * 对一个普通变量先写后读（两个线程分别做），写操作还没有完成，读操作会读到原来的值。这就会引发“不可见”的问题。volatile通过这条规则避免了不可见。

* 线程启动规则：**线程start()方法**一定先行发生于**此线程的每一个操作**；

* 线程终止规则：**线程的每一个操作**都一定先行发生于**对此线程的终止检测**；

* 线程中断规则：若线程被调用**interrupt()方法**，则此方法一定先行发生于**此线程的代码检测到中断事件的时刻**；

  * 啥意思呢，举个例子：

    ```java
    线程A：
    threadB.interrupt(); // (1)中断线程
    
    线程B：
    public void run() {
    	while(!Thread.interrupted()) { // (2)检测线程中断
    	}
    }
    ```

    在(2)语句返回true之前，一定是(1)先行发生。

* 对象终结规则：一个对象的**构造方法执行结束**，一定先行发生于**finalize()方法的开始**；

* 传递性：若操作A先行发生于操作B，操作B先行发生于操作C，就可以得出：操作A先行发生于操作C。



**注：对先行发生的定义**

“先行发生”与“时间上的先发生”不同。

“时间上的先发生”是指“时间顺序上优先被调用”，A先开始执行，B后开始执行，但可能A还没有执行完，B就开始执行了，如果B的执行依赖于A的结果，就会产生问题。

“先行发生”则是指，A先执行，等到A已经执行完了，才允许B开始执行。这叫A先行发生于B。这就不怕B依赖于A的结果了，反正A都执行完了。



## 5.3 volatile

当一个变量被定义为volatile之后，它具备两种特性：

* 保证此变量对所有线程的**可见性**；
* **禁止指令重排序**优化。

volatile保证了对变量的读、写操作的原子性（读写操作无法在一个没执行完的情况下就执行下一个），但是并不保证对变量的所有操作的原子性。比如对变量的递增（i++），这个操作就不是原子性的。因此这时使用volatile也会有线程安全问题。

### 3.3.1 可见性

volatile如何保证可见性？对于一个volatile变量：

* 被更新后，立即执行：store、write，将工作线程的副本立刻同步进主内存，同时使其他工作线程的副本失效；
* 被读取前，必须执行：read、load，将主内存的数据重新拷贝到线程自己的副本；
* volatile的先行发生原则：对一个volatile变量的写操作，一定先行发生于下一次读操作。

试想，如果只保证了前两条，仍然可能产生问题。想象这样一种情况：线程A先开始写入一个变量，还未写入完成时，B开始读。这时B读到的一定是旧数据，无法保证可见性。

![f05ce56540dde70da260dbb96c4379d.jpg](https://i.loli.net/2019/08/29/ajAU7TFCEIb9Gvi.jpg)



因此，先行发生原则中，又增加了一条“volatile变量规则”，对于一个volatile变量，必须写完了才允许读。



从8种原子性操作的角度来理解，volatile关键字的规则也可以描述成：

- read、load、use动作必须**连续出现**；
- assign、store、write动作必须**连续出现**。

这与上面的定义是一样的。

### 5.3.2 有序性

volatile利用一条内存屏障的指令来实现禁止重排序。上节讲的对volatile变量赋值流程如下：

```
（1）对变量赋值
（2）【内存屏障】执行store、write操作
```

由于store操作是将线程私有内存中的数据copy到主内存中去，因此这条操作一定依赖于赋值的结果——也就是说，（1）和（2）操作是有依赖关系的，（1）一定先行发生于（2）。

因此，执行到（2）语句时，意味着之前的赋值操作一定已经完成，这样便形成了“指令重排序无法越过内存屏障”的效果。



### 5.3.3 适用场景

由于volatile只能保证可见性和有序性，无法保证对变量多条操作的原子性（比如线程不安全的 `i++` ），因此相比于锁而言，它的适用场景稍稍偏窄。只有在满足如下两条时，才可以使用volatile：

1. 运算结果并不依赖变量当前的值，或者能够确保只有单一的线程修改变量的值；
   * 理解：因为如果依赖当前值的话，先计算再赋值，在没有其他锁的情况下，一定会有线程安全问题；
2. 该变量不需要与其他的变量共同纳入不变性条件中；
3. 在访问变量时不需要加锁。

举例：

```java
private static volatile boolean jump = false;

new Thread(() -> {
  while (!jump) {
  }
  System.out.println("jump out!");
}).start();

Thread.sleep(50);
jump = true;
```

分析：若jump变量没有volatile修饰，则最后的 `jump = true;` 语句将没有效果——对子线程不可见。因为内部线程早已经把jump的值拷贝到自己的私有内存中去了，每次循环都会命中私有内存，当然不会费事去主内存中读取。

jump加上volatile之后，执行 `jump = true;` 后，会立刻同步到主内存上去，并且把子线程中的jump值标记为失效。这样子下次下次读取时，发现本地内存中的jump值失效，则会去主内存中读取，就能读到修改后的值了。

## 5.4 synchronized

synchronized是重量级锁（还有很多其他称呼：悲观锁、互斥锁），volatile支持的特性它当然也会支持。除了可见性和禁止重排序外，还能保证操作的原子性。

### 5.4.1 原子性

通过加锁和解锁的方式实现原子性，即JVM提供的lock和unlock操作。

### 5.4.2 可见性

得到锁时，会从内存里重新读入数据（read、load），刷新缓存；

释放锁时，会将数据回写到内存（store、write），以保证对其他线程可见。

### 5.4.3 有序性

由于synchronized块内的代码一定是单线程执行，再结合先行发生原则中的“程序次序规则”，因此一定能保证有序性。



## 5.5 总结

* 缓存一致性问题：
  * CPU硬件层面，使用MESI协议解决；
  * Java内存模型层面，使用volatile解决；（在JMM中称为“可见性”）
* 重排序问题：
  * CPU硬件层面，CPU指令重排序时，分析语句间的依赖关系来保证结果正确；
  * Java内存模型层面，进行编译器重排序，volatile使用内存屏障来避免重排序问题。

| 关键字       | 原子性  |    可见性     |    有序性     |
| :----------- | :-----: | :-----------: | :-----------: |
| synchronzied | √（锁） | √（缓存失效） |  √（单线程）  |
| volatile     |         | √（缓存失效） | √（内存屏障） |





# 6. 最佳实践

**synchronized不要修饰web服务中的方法**

当synchronized修饰方法时，锁住的是这个实例对象。而对于基于spring的web服务而言，默认都是单例的bean，一旦锁住了这个对象，就说明该方法无法并发服务于多个用户，只能服务完一个用户，这个bean被解锁后，下一个用户的请求才能获取到这个bean，这对web服务的体验带来毁灭性的打击。

其次，对于多核服务器，锁住了一个对象，该对象就无法被其他CPU使用了，造成一人干活，多人围观的情况。同样会降低程序的并发性。

















# 附录

## 一、2019.9.15期分享大纲

- 线程安全的五个级别
  - Java中对应的哪些级别，分别举例
- 如何实现线程安全？
  - 【插播】操作系统相关知识
    - 用户态与内核态；
    - 线程切换、阻塞与唤醒的性能问题；
  - 互斥（非阻塞）同步：悲观锁
    - synchronized
    - ReentrantLock
    - 二者对比，优缺点
  - 非阻塞同步：乐观锁
    - CAS操作，及其ABA问题
  - 无同步方案
    - 完全在栈上分配内存；
    - ThreadLocal；
- 锁优化
  - 自旋锁；
  - 适应性自旋；
  - 锁消除；
  - 锁粗化；
  - 轻量级锁；
  - 偏向锁；



- CPU的并发问题
  - 【插播】CPU的l1、l2缓存，进程本地内存；
  - 缓存一致性问题，与缓存一致性协议；
  - 重排序优化问题，与代码依赖关系分析；
- Java内存模型
  - JMM逻辑模型与堆栈内存的关系；
- JVM并发问题的解决
  - 8种原子性操作
  - 8个先行发生原则
  - volatile如何保证可见性、有序性？
  - synchronized如何保证原子性、可见性、有序性？